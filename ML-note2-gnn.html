<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="code-dWHnANgGFW"><link rel="apple-touch-icon" sizes="76x76" href="/img/about_me.jpg"><link rel="icon" href="/img/about_me.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="CindyWen"><meta name="keywords" content=""><meta name="description" content="本文试图沿傅里叶变换阐释图神经网络中的数学原理，并按照分 Spatial-based、Spectral-based 两类阐释模型结构。"><meta property="og:type" content="article"><meta property="og:title" content="ML学习笔记 #02 GNN"><meta property="og:url" content="https://yiwen-ding.github.io/ML-note2-gnn"><meta property="og:site_name" content="CindyWen"><meta property="og:description" content="本文试图沿傅里叶变换阐释图神经网络中的数学原理，并按照分 Spatial-based、Spectral-based 两类阐释模型结构。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://yiwen-ding.github.io/img/blog/ML-note2-gnn/gnn.png"><meta property="article:published_time" content="2022-11-29T12:27:45.000Z"><meta property="article:modified_time" content="2022-11-29T12:59:09.527Z"><meta property="article:author" content="CindyWen"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://yiwen-ding.github.io/img/blog/ML-note2-gnn/gnn.png"><title>ML学习笔记 #02 GNN - CindyWen</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"yiwen-ding.github.io",root:"/",version:"1.9.3",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>CindyWen&#39;s blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/post_bg.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">ML学习笔记 #02 GNN</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-11-29 20:27" pubdate>2022年11月29日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 17k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 143 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">ML学习笔记 #02 GNN</h1><div class="markdown-body"><h1 id="图神经网络-graph-neural-network">图神经网络 | Graph Neural Network</h1><h2 id="why-gnn">1 Why gnn?</h2><ul><li><p>在实际应用中， 很多数据是图结构的， 比如知识图谱、 社交网络、 分子网络等。但前馈网络和反馈网络很难处理图结构的数据</p></li><li><p>考虑 entity feature 的同时还考虑 entity 之间 的<strong>空间特征</strong>，即 relation 和 structure</p></li><li><p>困难点在于：</p><p>CNN 的<strong>局部平移不变性</strong>，可以很好运用于图片中的规整的二维矩阵 image grids。所谓的「局部平移不变性」即通过卷积操作类似于对<strong>「计算区域内的中心节点和相邻节点进行加权求和」</strong>，无论卷积核平移到图片中的哪个位置都可以保证运算结果的一致性。而图是任意大小的复杂结构，并非是规整的（每个节点的周围邻居数不固定），不能直接使用 CNN。</p><p><img src="img/blog/ML-note2-gnn/o_image-9-cnn-and-gcn.png" srcset="/img/loading.gif" lazyload alt="GGNN语义解析实例" style="zoom:67%"></p></li></ul><h2 id="前置知识-preliminary">2 前置知识 | Preliminary</h2><h3 id="定义-basic-definitions">2.1 定义 | Basic Definitions</h3><p>首先对图中节点、边进行定义，主要包括邻接矩阵和度矩阵。</p><ul><li>Graph: <span class="math inline">\(G=(V, E), N=|V|\)</span></li><li><span class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span>：<strong>「邻接矩阵（Adjacency Matrix）」</strong>, 表示节点之间连接的权重大小，也可以写作 <span class="math inline">\(W\)</span></li></ul><p><span class="math display">\[ A_{i, j}=0 \text { if } e_{i, j} \notin E \text {, else } A_{i, j}=w(i, j) \]</span></p><ul><li><span class="math inline">\(D \in \mathbb{R}^{N \times N}\)</span>: <strong>「度矩阵（Degree Matrix）」</strong>，为对角矩阵，表示当前节点的度数之和</li></ul><p><span class="math display">\[ D_{i, j}=\left\{\begin{array}{ll} d(i) &amp; \text { if } i=j \quad \text { (Sum of row } i \text { in } A) \\ 0 &amp; \text { if } i \neq j \end{array} \quad\right. \]</span></p><ul><li><p><span class="math inline">\(f: V \rightarrow \mathbb{R}^N\)</span>：<strong>「图信号（signal on graph - vertex）」</strong> <span class="math inline">\(f(i)\)</span> denotes the signal on vertex <span class="math inline">\(i\)</span></p><figure><img src="img/blog/ML-note2-gnn/image-20221121194423741.png" srcset="/img/loading.gif" lazyload alt="image-20221121194423741"><figcaption>image-20221121194423741</figcaption></figure></li><li><p><span class="math inline">\(L=D-A, L \geqslant 0\)</span>：<strong>「拉普拉斯矩阵（Laplacian Matrix）」</strong>是<strong>半正定对称矩阵</strong></p><ul><li><span class="math inline">\(L\)</span> 是对称的，实对称矩阵一定可以用<strong>正交矩阵</strong>进行正交相似对角化： <span class="math inline">\(L=\Phi \Lambda \Phi^{-1}=\Phi \Lambda \Phi^{\mathrm{T}}\)</span></li><li>矩阵特征值<span class="math inline">\(\Lambda=\operatorname{diag}\left(\lambda_0, \ldots, \lambda_{N-1}\right) \in \mathbb{R}^{N \times N}\)</span><strong>一定非负</strong></li><li>根据实对称矩阵<strong>不同特征值</strong>所对应的<strong>特征向量必定正交</strong>。 <span class="math inline">\(\Phi=\left[\phi_0, \ldots, \phi_{N-1}\right] \in \mathbb{R}^{N \times N}\)</span>是正交矩阵 <span class="math inline">\(\lambda_l\)</span> 代表频率， <span class="math inline">\(\phi_l\)</span> 代表 <span class="math inline">\(\lambda_l\)</span> 对应的基向量</li></ul><p><img src="img/blog/ML-note2-gnn/image-20221121202032720.png" srcset="/img/loading.gif" lazyload alt="image-20221121202032720" style="zoom:50%"></p><ul><li>通过图信号非0向量 <span class="math inline">\(f\)</span> 计算可得到光滑程度。根据「若对于每个非零实向量<span class="math inline">\(X\)</span>，都有 <span class="math inline">\(X'AX\ge0\)</span>，则称A为<strong>半正定矩阵</strong>」的定理，可知矩阵 <span class="math inline">\(L\)</span> 是<strong>半正定</strong>的。</li></ul><p><span class="math display">\[ \begin{aligned} f^{T}Lf &amp;=f^{T}(Df - Af) \\ &amp;=\sum_{v_i \in V} f\left(v_i\right) \sum_{v_j \in V} w_{i, j}\left(f\left(v_i\right)-f\left(v_j\right)\right) \\ &amp;=\sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f^2\left(v_i\right)-f\left(v_i\right) f\left(v_j\right)\right) \\ &amp;=\frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f^2\left(v_i\right)-f\left(v_i\right) f\left(v_j\right)+f^2(j)-f\left(v_j\right) f\left(v_i\right)\right) \\ &amp;=\frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f\left(v_i\right)-f\left(v_j\right)\right)^2 \ge0 \quad \begin{array}{l} \text { "Power" of signal variation } \\ \text { between nodes, i.e., } \\ \text { smoothness of graph signal } \end{array} \end{aligned} \]</span></p></li></ul><h3 id="拉普拉斯算子-laplacian">2.2 拉普拉斯算子 | Laplacian</h3><p>图拉普拉斯矩阵<span class="math inline">\(L\)</span> 为什么要这样定义的？首先得了解什么是拉普拉斯算子。</p><p>在数学中，拉普拉斯算子 (Laplacian) 是由欧几里得空间中的一个函数的梯度的<strong>散度</strong>给出的<strong>微分算子</strong>，通常有以下几种写法: <span class="math inline">\(\Delta, \nabla^2, \nabla \cdot \nabla\)</span> 。所以对于任意函数 <span class="math inline">\(f\)</span> 来说，其拉普拉斯算子的定义为: <span class="math display">\[ \Delta f=\nabla^2 f=\nabla \cdot \nabla f\\ =\sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2} \]</span></p><p>以一维空间为例： <span class="math display">\[ \begin{aligned} \frac{\partial^2 f}{\partial x_i^2} &amp;=f^{\prime \prime}(x) \\ &amp; \approx f^{\prime}(x)-f^{\prime}(x-1) \\ &amp; \approx f(x+1)-f(x)-(f(x)-f(x-1)) \\ &amp;=f(x+1)+f(x-1)-2 f(x) \end{aligned} \]</span> 也就是说二阶导数近似于其二阶差分，可以理解为当前点对其在所有自由度上微扰之后获得的增益。这里自由度为 2，分别是 +1 和 -1 方向。</p><p>再以二维空间为例子：此时共有 4 个自由度 (1,0),(-1,0),(0,1),(0,-1)，当然如果对角线后其自由度可以为 8。 <span class="math display">\[ \begin{aligned} \Delta f(x, y) &amp;=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2} \\ &amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\ &amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y) \end{aligned} \]</span> 归纳可得，<strong>「拉普拉斯算子是所有自由度上进行微小变化后所获得的增益」</strong>。</p><p>推广到网络图中，考虑有 N 个节点的网络图，其自由度最大为 N，那么函数 <span class="math inline">\(f\)</span> 可以是 N 维的向量，即： <span class="math display">\[ f= (f_1,...f_N) \]</span> 其中 <span class="math inline">\(f_i\)</span> 表示在网络图中节点 <span class="math inline">\(i\)</span> 处的函数值。</p><p>在网络图中，两个节点的之间的增益为 <span class="math inline">\(f_i-f_j\)</span> ，考虑加权图则有 <span class="math inline">\(w_{i j}\left(f_i-f_j\right)\)</span> ，那么对于节点 <span class="math inline">\(i\)</span> 来说，总增益即为拉普拉斯算子在节点 <span class="math inline">\(i\)</span> 的值: <span class="math display">\[ \begin{aligned} \Delta \boldsymbol{f}_i &amp;=\sum_{j \in N_i} \frac{\partial f_i}{\partial j^2} \\ &amp; \approx \sum_j w_{i j}\left(f_i-f_j\right) \\ &amp;=\sum_j w_{i j}\left(f_i-f_j\right) \\ &amp;=\left(\sum_j w_{i j}\right) f_i-\sum_j w_{i j} f_j \\ &amp;=d_i f_i-w_{i:} f_i \end{aligned} \]</span> 其中， <span class="math inline">\(d_i=\sum_{j \in N_i} w_{i j}\)</span> 为节点 <span class="math inline">\(i\)</span> 的度；上式第二行去掉了 <span class="math inline">\(j \in N_i\)</span> 是因为 <span class="math inline">\(w_{i j}\)</span> 可以控制节点 <span class="math inline">\(i\)</span> 的邻接矩阵。 对于任意 <span class="math inline">\(i \in N\)</span> 都成立，所以有: <span class="math display">\[ \begin{aligned} \Delta f=\left(\begin{array}{c} \Delta f_1 \\ \vdots \\ \Delta f_N \end{array}\right) &amp;=\left(\begin{array}{cc} d_1 f_1-w_{1:} f \\ \vdots \\ d_N f_N-w_{N:} f \end{array}\right) \\ &amp;=\left(\begin{array}{ccc} d_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; d_N \end{array}\right) f-\left(\begin{array}{c} w_{1:} \\ \vdots \\ w_{N:} \end{array}\right) f \\ &amp;=\operatorname{diag}\left(d_i\right) f-\mathbf{W} f \\ &amp;=(\mathbf{D}-\mathbf{W}) f \\ &amp;=\mathbf{L} f \end{aligned} \]</span> 这个公式的全称为：图拉普拉斯算子作用于 <span class="math inline">\(f\)</span> 上的结果等于「<strong>图拉普拉斯矩阵和向量 <span class="math inline">\(f\)</span> 的点积</strong>」。<span class="math inline">\(L\)</span> 也直接称作图拉普拉斯算子，反映了当前节点<strong>对周围节点</strong>产生扰动时所产生的累积增益，直观上也可以理解为某一节点的权值变为其相邻节点权值的期望影响，可以刻画局部的<strong>平滑度</strong>。</p><h3 id="傅里叶级数-fourier-series">2.3 傅里叶级数 | Fourier Series</h3><ul><li>定义：参考用函数的<strong>「幂级数展开式」</strong>表示与讨论函数的思想，傅里叶级数尝试将<strong>非正弦周期</strong>函数展开成三角函数组成的级数。具体地说，将周期为 <span class="math inline">\(T\left(=\frac{2 \pi}{\omega}\right)\)</span> 的周期函数用一系列以 <span class="math inline">\(T\)</span> 为周期的正弦函数 <span class="math inline">\(A_n \sin \left(n \omega t+\varphi_n\right)\)</span> 组成的级数来表示，记为</li></ul><p><span class="math display">\[ f(t)=A_0+\sum_{n=1}^{\infty} A_n \sin \left(n \omega t+\varphi_n\right), \]</span></p><p>其中 <span class="math inline">\(A_0, A_n, \varphi_n(n=1,2,3, \cdots)\)</span> 都是常数。将其中的 sin 正弦函数展开，并且令 <span class="math inline">\(\frac{a_0}{2}=A_0,a_n=A_n \sin \varphi_n,\)</span> <span class="math inline">\(b_n=A_n \cos \varphi_n, \omega=\frac{\pi}{l}(\)</span> 即 <span class="math inline">\(T=2 l)\)</span>，式子可改写为 <span class="math display">\[ \begin{aligned} f(t)&amp;=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left(a_n \cos \frac{n \pi t}{l}+b_n \sin \frac{n \pi t}{l}\right)\\ &amp;=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left(a_n \cos n x+b_n \sin n x\right) \;given \;\frac{\pi t}{l}=x \end{aligned} \]</span></p><ul><li><p>物理含义：将一个复杂的周期运动看成是许多<strong>不同频率</strong>的简谐振动的<strong>叠加</strong>，其中不同的振荡函数具有不同的振幅和频率。</p><p>以 <span class="math inline">\(f(t) \approx 2.5+\frac{10}{\pi}\left(\sin \frac{\pi t}{4}+\frac{1}{3} \sin \frac{3 \pi t}{4}+\frac{1}{5} \sin \frac{5 \pi t}{4}+\frac{1}{7} \sin \frac{7 \pi t}{4}\right)\)</span> 为例，考虑以频率为横坐标，振幅为纵坐标，可绘制频域函数图如下。</p><p><img src="img/blog/ML-note2-gnn/v2-0b06eb050563bdfdd8bdde88e98757c1_r.jpg" srcset="/img/loading.gif" lazyload alt="img" style="zoom:80%"></p><p>从下图可感受<strong>时域信息到频域信息的转化</strong>。</p><ul><li>时域：时间和振幅的关系图，横坐标是<strong>时间</strong>，纵坐标是振幅。</li><li>频域：频率和振幅的关系图，横坐标是<strong>频率</strong>，纵坐标是振幅。</li></ul><p><img src="img/blog/ML-note2-gnn/v2-dace79414a915197e605b7b1fde36ba0_720w.webp" srcset="/img/loading.gif" lazyload alt="img" style="zoom:67%"></p><figure><embed src="img/blog/ML-note2-gnn/v2-38d2e076c57f6d3dd28749c77c89ed6e_720w.webp"><figcaption>img</figcaption></figure></li><li><p>三角函数系：</p><p>正交基满足 <span class="math display">\[ \begin{aligned} &amp;\begin{cases} \int_{-\pi}^{\pi}{\cos}(mx)\cos\mathrm{(}nx)dx=\pi ,&amp; m=n,m,n\ge 1\\ \int_{-\pi}^{\pi}{\cos}(mx)\cos\mathrm{(}nx)dx=0,&amp; m\ne n,m,n\ge 1\\ \end{cases}\\ &amp;\begin{cases} \int_{-\pi}^{\pi}{\sin}(mx)\sin\mathrm{(}nx)dx=\pi ,&amp; m=n,m,n\ge 1\\ \int_{-\pi}^{\pi}{\sin}(mx)\sin\mathrm{(}nx)dx=0,&amp; m\ne n,m,n\ge 1\\ \end{cases}\\ &amp;\int_{-\pi}^{\pi}{\cos}(mx)\sin\mathrm{(}nx)dx=0,\quad m,n\ge 1\\ &amp;\int_{-\pi}^{\pi}{\cos}(nx)dx=0,\quad n\ge 1\\ &amp;\int_{-\pi}^{\pi}{\sin}(nx)dx=0,\quad n\ge 1\\ \end{aligned} \]</span></p></li></ul><p>​ 因此，<span class="math inline">\(1,\;cos x,\;sin x,\;cos 2x,\;sin 2x,\;cos nx,\;sin nx\)</span> 构成一组<strong>标准正交基</strong>。任何不同的两个函数的乘积在区间 <span class="math inline">\([-T，T]\)</span> 上的积分等于 0。</p><ul><li><p>系数求解：</p><p>通过将右端的级数逐项积分，等式两端同时乘以对应三角函数，利用正交性抵消，求解系数 <span class="math inline">\(a_0, a_1, b_1\cdots\)</span> ，称其为<strong>「傅里叶级数」</strong>。</p></li></ul><h3 id="傅里叶变换-fourier-transformer">2.4 傅里叶变换 | Fourier Transformer</h3><p>前述的傅里叶级数适用于<strong>非正弦周期</strong>函数的变换，但现实中大部分函数都是非周期的，接下来讨论涉及到非周期性函数的方法。</p><p>在介绍非周期性函数之前，首先介绍下<strong>欧拉公式</strong>。</p><p>考虑横轴为 1，纵轴为虚单位 i 的坐标系，图上任意一点都可以表示为 <span class="math inline">\(cos\theta + isin\theta\)</span>，根据欧拉公式，可以写成： <span class="math inline">\(cos\theta + isin\theta = e^{i\theta}\)</span>，其含义为<strong>找到权重为正余弦的两个正交基（实部和虚部）</strong></p><p><img src="img/blog/ML-note2-gnn/euler.png" srcset="/img/loading.gif" lazyload alt="euler" style="zoom:60%"></p><p>以时间 t 为横坐标，则可以记录到坐标点 A 映射在虚轴的运动轨迹，图中体现了频域和时域的相互转化。</p><p><img src="img/blog/ML-note2-gnn/iwt.png" srcset="/img/loading.gif" lazyload alt="iwt" style="zoom:67%"></p><p>回到非周期函数的傅立叶变换的讨论，可以将非周期函数考虑为周期无穷大的函数，考虑频域中的横坐标：<span class="math inline">\(f=\frac{1}{T}\)</span>，当周期 T 无穷大时，频域图就从离散点变为连续的曲线，如下图所示。</p><p><img src="img/blog/ML-note2-gnn/fourier_transform_jishu.png" srcset="/img/loading.gif" lazyload alt="fourier_transform_jishu" style="zoom:67%"></p><ul><li><strong>复数形式</strong>：傅里叶级数展开公式中有正弦波，也有余弦波，画频域图也不方便，<strong>通过欧拉公式，可以修改为复数形式</strong>：</li></ul><p><span class="math display">\[ f(x)=\sum_{n=-\infty}^{\infty} c_n \cdot e^{i \frac{2 \pi n}{T} x} \]</span></p><ul><li><strong>逆傅里叶变换公式</strong> 如下：复数形式也可以理解为向量， <span class="math inline">\(e^{i \frac{2 \pi n}{T} x}\)</span> 是基， <span class="math inline">\(c_n\)</span> 是该基下的坐标。从数学角度， <span class="math inline">\(f(x)\)</span> 是函数 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(x\)</span> 处的取值，所有基都对该处取值有贡献，即把每个 <span class="math inline">\(F(w)\)</span> 投影 到 <span class="math inline">\(e^{i w x}\)</span> 基方向上分量累加起来，得到的就是该点处的函数值。</li></ul><p><span class="math display">\[ f(x)=\int_{-\infty}^{\infty} F(w) e^{i w x} d w=\sum_w F(w) e^{i w x} \]</span></p><ul><li><strong>傅立叶变换</strong> (Fourier Transform，FT) 公式如下，其中， <span class="math inline">\(e^{-i w t}\)</span> 是一组正交基的组合。「实数部分表示振幅」，「虚数部分表示相位」。从数学角度， <span class="math inline">\(F(w)\)</span> 就是每个基下对应的坐标值，所有的 <span class="math inline">\(x\)</span> 对该基都有贡献，即把每个<span class="math inline">\(f(x)\)</span> 投影到 <span class="math inline">\(e^{-iwx}\)</span> 基方向上的分量全部累加起来，得到的就是该基方向的坐标值。<strong>本质上是将函数 <span class="math inline">\(f(x)\)</span> 映射到了以 <span class="math inline">\(e^{-iwx}\)</span> 为基向量的空间中。</strong></li></ul><p><span class="math display">\[ F(w)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} f(x) e^{-iwx} d x \]</span></p><ul><li><strong>相互转换：</strong></li></ul><p><span class="math display">\[ f(x)⇔F(w) \]</span></p><p>一个是函数，一个是<strong>向量</strong>（频域曲线纵坐标构成的向量，基为频域曲线横坐标对应的的基函数）。</p><h3 id="图的傅里叶变换">2.5 图的傅里叶变换</h3><ul><li>传统的傅里叶变换</li></ul><p><span class="math display">\[ F(k)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} f(x) e^{-i k x} d x \approx\left\langle f, e^{-i k x}\right\rangle \]</span></p><p><span class="math inline">\(F(k)\)</span> 是傅里叶系数（即频率为 <span class="math inline">\(k\)</span> 时的振幅值）。约等号去掉了常数系数，同时 <span class="math inline">\(x\)</span> 为离散变量 时，离散积分等于内积。<span class="math inline">\(e^{-i k x}\)</span> 为 Fourier Basis。可以证明 <span class="math inline">\(e^{-i k x}\)</span> 是<strong>拉普拉斯算子的特征函数</strong>（满足特征方程 <span class="math inline">\(A V=\lambda V)\)</span> ，证明: <span class="math display">\[ \Delta e^{-i k x}=\frac{\partial e^{-i k x}}{\partial x^2}=-k^2 e^{-i k x} \]</span></p><ul><li>在Graph上作类比，设 <span class="math inline">\(\phi_k\)</span> 是图拉普拉斯算子 <span class="math inline">\(\boldsymbol{L}\)</span> 的特征向量，（满足 <span class="math inline">\(\boldsymbol{L} \phi_k=\lambda_k \phi_k\)</span> ）。在 Graph中， <span class="math inline">\(\Delta=\boldsymbol{L}\)</span>, <span class="math inline">\(e^{-i k x}=\phi_{\boldsymbol{k}}\)</span>。因此，为了在Graph上进行傅里叶变换，可以把传统傅里叶变换中的基 <span class="math inline">\(e^{-i k x}\)</span> 换成 <span class="math inline">\(\phi_{\boldsymbol{k}}\)</span> （是<strong>线性无关的正交向量</strong>）。 <span class="math display">\[ F(\lambda_k)=\left\langle f, \phi_\boldsymbol{k}\right\rangle\\ \]</span> 矩阵形式的<strong>图傅里叶变换</strong>： <span class="math display">\[ \hat{\boldsymbol{f}} = \boldsymbol{\Phi}^T \boldsymbol{f} \]</span> <strong>迁移到Graph上的逆傅里叶变换</strong> <span class="math display">\[ f_i= \sum_{k=1}^n \hat{f}_k (\boldsymbol{\phi_{k}}^T )_i \]</span> 推广到矩阵形式， <span class="math display">\[ \boldsymbol{f} = \boldsymbol{\Phi} \boldsymbol{\hat{f}} \]</span></li></ul><table><thead><tr class="header"><th style="text-align:left">传统傅里叶变换</th><th style="text-align:left">图傅里叶变换</th></tr></thead><tbody><tr class="odd"><td style="text-align:left">频率（<span class="math inline">\(k\)</span>）</td><td style="text-align:left">特征值（<span class="math inline">\(\lambda_k\)</span>）</td></tr><tr class="even"><td style="text-align:left">正弦函数 <span class="math inline">\(e^{-i k x}\)</span></td><td style="text-align:left">特征向量 <span class="math inline">\(\phi_\boldsymbol{k}\)</span></td></tr><tr class="odd"><td style="text-align:left">振幅 <span class="math inline">\(F(k)\)</span></td><td style="text-align:left">振幅 <span class="math inline">\(F(\lambda_k)\)</span></td></tr></tbody></table><h2 id="gnn-模型架构-model">3 GNN 模型架构 | Model</h2><p>GNN 理论基础是<strong>不动点</strong>理论，被划分成 <strong>空域（Spatial-based GNN）</strong> 和 <strong>谱域（Spectral-based GNN）</strong> 两大类，空域模型不需要矩阵特征分解，能直接在空域视角进行矩阵计算；谱域模型则从信号处理的角度实现 GNN。</p><h3 id="状态更新与输出">3.1 <strong>状态更新与输出</strong></h3><p>在一个图结构中，<strong>每一个节点由它自身的特征以及与其相连的节点特征来定义该节点</strong>。GNN的目标是学习得到每个结点的图感知的隐藏状态 <span class="math inline">\(\mathbf{h}_v\)</span> (state embedding)</p><p>GNN通过迭代式更新所有结点的隐藏状态来实现，在 <span class="math inline">\(t+1\)</span> 时刻，结点 <span class="math inline">\(v\)</span> 的隐藏状态按照如下 方式更新: <span class="math display">\[ \begin{aligned} \mathbf{h}_v^{t+1}&amp;=f\left(\mathbf{x}_v, \mathbf{x}_c o[v], \mathbf{h}_n^t e[v], \mathbf{x}_n e[v]\right)\\ o_v &amp;= g(h_v,x_v) \end{aligned} \]</span> 用 <span class="math inline">\(\mathbf{x}_v\)</span> 表示结点 <span class="math inline">\(\mathrm{V}\)</span> 的特征；连接两个结点的边也有自己的特征， <span class="math inline">\(\mathbf{x}_{(v, u)}\)</span> 表示结点 <span class="math inline">\(\mathrm{v}\)</span> 与结点 <span class="math inline">\(\mathrm{u}\)</span> 之间边的特征。</p><ul><li><span class="math inline">\(f\)</span> 就是隐藏状态的状态更新函数，也被称为<strong>局部转移函数</strong> (local transaction function)<strong>，这个函数在所有节点中共享，带有参数</strong>。</li><li>公式中的 <span class="math inline">\(\mathbf{x}_c o[v]\)</span> 指 的是与结点 <span class="math inline">\(v\)</span> 相邻的边的特征</li><li><span class="math inline">\(\mathbf{x}_n e[v]\)</span> 指的是结点 <span class="math inline">\(v\)</span> 的邻居结点的特征</li><li><span class="math inline">\(\mathbf{h}_n^t e[v]\)</span> 则指邻居结点在 <span class="math inline">\(t\)</span> 时刻的隐藏状态。</li><li><span class="math inline">\(g\)</span> 又被称为<strong>局部输出函数</strong>(local output function)，用于描述输出的产生方式。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-2-state-update-function.png" srcset="/img/loading.gif" lazyload alt="更新公式示例" style="zoom:33%"></p><blockquote><p>更新公式的含义：不断地利用当前时刻邻居结点的隐藏状态作为部分输入来生成下一时刻中心结点的隐藏状态，直到每个结点的隐藏状态变化幅度很小，整个图的信息流动趋于平稳。至此，每个结点都“知晓”了其邻居的信息。</p></blockquote><p>更新至收敛的条件：通过两个时刻 <span class="math inline">\(p\)</span>范数的差值是否小于某个阈值 <span class="math inline">\(\epsilon\)</span> 来判定的 <span class="math display">\[ ||H^{t+1}||_2−||H^t||_2&lt;\epsilon \]</span></p><p>用 <span class="math inline">\(F\)</span> 表示若干个 <span class="math inline">\(f\)</span> <strong>堆叠</strong>得到的一个函数，也称为<strong>全局更新</strong>函数，那么图上所有结点的状态更新公式可以写成： <span class="math display">\[ H^{t+1}=F(H^t,X) \]</span></p><ul><li><strong>不动点</strong>(the fixed point)理论指的是：只要 <span class="math inline">\(F\)</span> 是个<strong>压缩映射</strong>(contraction map)， <span class="math inline">\(H^0\)</span> 经过不断迭代都会收敛到某一个固定的点，称之为不动点。</li><li><strong>压缩映射 </strong>的含义：任意两个点 <span class="math inline">\(x, y\)</span> 在经过 <span class="math inline">\(F\)</span> 映射后变为 <span class="math inline">\(F(x), F(y)\)</span> 。 <span class="math inline">\(d(F(x), F(y)) \leq c d(x, y), 0 \leq c&lt;1\)</span> 。 经过 <span class="math inline">\(F\)</span> 变换后的新空间一定比原先的空间要小，原先的空间被压缩了。想象这种压缩的过程不断进行，最终就会把原空间中的所有点映射到一个点上。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-3-contraction-map.png" srcset="/img/loading.gif" lazyload alt="更新公式示例" style="zoom:50%"></p><h3 id="模型学习">3.2 模型学习</h3><p>通过模型学习，让 <span class="math inline">\(f\)</span> 接近压缩映射。使用目标信息（<span class="math inline">\(t_v\)</span>表示特定节点的标签）来进行监督学习，loss定义如下： <span class="math display">\[ loss=\sum_{i = 1}^{p}(t_i - o_i) \]</span> 根据 <strong>前向传播计算损失、反向传播计算梯度</strong> 进行学习</p><ul><li>状态 <span class="math inline">\(h_v^t\)</span> 按照迭代方程更新 <span class="math inline">\(T\)</span> 个轮次，直至收敛。</li><li>对于有监督信号的结点，将其隐藏状态通过 <span class="math inline">\(g\)</span> 得到输出，进而算出模型的损失。</li><li>得到的 <span class="math inline">\(H\)</span> 会接近不动点的解 $H(T)≈H $。</li><li>反向传播计算 <span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span> 对 <span class="math inline">\(h_v^0\)</span> 的梯度，用于更新模型的参数。</li></ul><h3 id="gnn-vs-rnn">3.3 GNN vs RNN</h3><p>将循环神经网络与图神经网络对比，存在以下不同点：</p><p>假设在GNN中存在三个结点 <span class="math inline">\(x_1, x_2, x_3\)</span>，在RNN中有一个序列 <span class="math inline">\((x_1, x_2, x_3)\)</span>。</p><ul><li><strong>输入：</strong>GNN每次时间步的输入都是<strong>所有结点</strong> <span class="math inline">\(v\)</span> 的特征，而 RNN 每次时间步的输入是<strong>该时刻</strong>的输入。同时，时间步之间的信息流也不相同，前者由边决定，后者则由序列的读入顺序决定。</li><li><strong>训练过程：</strong>GNN的基础理论是不动点理论，因此其沿时间展开的长度是动态的，<strong>根据收敛条件</strong>确定的，而RNN沿时间展开的长度就等于<strong>序列本身的长度</strong>。</li><li><strong>训练目标：</strong>GNN目标是得到每个结点<strong>稳定的隐藏状态</strong>，只有在隐藏状态收敛后才能输出；而RNN的每个时间步上都可以输出，比如语言模型。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-5-gnn-rnn.png" srcset="/img/loading.gif" lazyload alt="GNN与RNN的区别" style="zoom:50%"></p><h3 id="局限性">3.4 局限性</h3><p>原始 GNN 的核心观点是<strong>通过结点信息的传播使整张图达到收敛，在其基础上再进行预测</strong>。问题在于：</p><ul><li>GNN只将边作为一种传播手段，但并未区分不同边的功能。虽然可以在特征构造阶段 <span class="math inline">\((x_{(u,v)})\)</span> 为不同类型的边赋予不同的特征，但相比于其他输入，边对结点隐藏状态的影响有限。并且GNN没有为边设置独立的可学习参数，无法通过模型学习到边的某些特性。</li><li>GNN应用在 <strong>节点表示</strong> 的场景中，使用不动点理论并不合适。因为基于不动点的收敛会导致结点之间的隐藏状态间存在较多信息共享，<strong>不动点的向量表示分布在数值上会非常的平滑</strong>，属于结点自身的特征<strong>信息匮乏</strong>。</li></ul><h2 id="spatial-based-gnn">4 Spatial-based GNN</h2><p>由于传统的卷积不能直接用在图上，主要的难点在于<strong>邻居结点数量不固定</strong>。</p><ul><li><p>思想：</p><ul><li><p>aggregate: 用 neighbor feature 更新下一层的 hidden state</p></li><li><p>readout: 将所有 nodes 的 feature 集合起来代表整个 graph</p><p><img src="img/blog/ML-note2-gnn/image-20221114225603657.png" srcset="/img/loading.gif" lazyload alt="image-20221114225603657" style="zoom:67%"></p></li></ul></li><li><p>具体的聚合方式</p></li></ul><table><thead><tr class="header"><th>Aggregation</th><th>Method</th></tr></thead><tbody><tr class="odd"><td>Sum</td><td>NN4G</td></tr><tr class="even"><td>Mean</td><td>DCNN, DGC, GraphSAGE</td></tr><tr class="odd"><td>Weighted sum</td><td>MoNET, GAT, GIN</td></tr><tr class="even"><td>LSTM</td><td>GraphSAGE</td></tr><tr class="odd"><td>Max Pooling</td><td>GraphSAGE</td></tr></tbody></table><h3 id="消息传递网络-mpnn">4.1 消息传递网络 | MPNN</h3><p>消息传递网络（Message Passing Neural Network）将空域卷积分解为两个过程：<strong>消息传递</strong>与<strong>状态更新</strong>操作，分别由$M_l(⋅) $ 和 <span class="math inline">\(U_l(⋅)\)</span> 函数完成。将结点 <span class="math inline">\(v\)</span> 的特征 <span class="math inline">\(x_v\)</span> 作为其隐藏状态的初始态 <span class="math inline">\(h_0^v\)</span> 后，空域卷积对隐藏状态的更新由如下公式表示： <span class="math display">\[ \mathbf{h}_{v}^{l+1}=U_{l+1}(\mathbf{h}_v,\sum_{u{\in}ne[v]}M_{l+1}(\mathbf{h}_v^l,\mathbf{h}_u^l,\mathbf{x}_{vu})) \]</span> 其中 <span class="math inline">\(l\)</span> 代表图卷积的第 <span class="math inline">\(l\)</span> 层，上式的物理意义是：收到来自每个邻居的的消息 <span class="math inline">\(M_{l+1}\)</span> 后，每个结点如何更新自己的状态。</p><h3 id="graphsage">4.2 GraphSAGE</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.02216.pdf">GraphSAGE</a> 全称为 Graph Sample and Aggregate。区别于传统的全图卷积（将所有结点放入内存/显存中），GraphSAGE 利用<strong>采样</strong> (Sample) 部分结点的方式进行学习。当然，即使不需要整张图同时卷积，GraphSage仍然需要聚合邻居结点的信息，即 <em>aggregate</em> 的操作。这种操作类似于 MPNN 中的<strong>消息传递</strong>过程。</p><p><img src="img/blog/ML-note2-gnn/image-20221124171720451.png" srcset="/img/loading.gif" lazyload alt="image-20221124171720451" style="zoom:67%"></p><ul><li><p>采样过程</p><ul><li>在图中随机<strong>采样</strong>若干个结点，结点数为 <code>batch_size</code>。对于每个结点，随机选择固定数目的邻居结点（一阶邻居，或是二阶邻居）构成进行卷积操作的图。</li><li>将邻居结点的信息通过 aggregate 函数聚合起来更新刚才采样的结点。</li><li>计算采样结点处的损失。如果是无监督任务，希望图上邻居结点的编码相似；如果是监督任务，可根据具体结点的任务标签计算损失。</li></ul></li><li><p>更新公式为 <span class="math display">\[ \mathbf{h}_{v}^{l+1}=\sigma(\mathbf{W}^{l+1}\cdot aggregate(\mathbf{h}_v^l,\{\mathbf{h}_u^l\}),{\forall}u{\in}ne[v]) \]</span></p></li><li><p>聚类器</p><p>GraphSAGE 的设计重点就放在了 aggregate 函数的设计上。它可以是不带参数的max，mean，也可以是带参数的如 LSTM 的等神经网络。核心的原则是需要可以处理变长的数据。</p></li><li><p>GraphSAGE 适用于 Inductive learning，扩展到新的节点和新的图，因为直接学习的采样过程和加权求和方式，并没有利用图的拉普拉斯矩阵。</p></li></ul><h2 id="spectral-based-gnn">5 Spectral-based GNN</h2><h3 id="动机">5.1 动机</h3><ul><li><p><strong>动机：</strong>既然无法直接在时域进行卷积，就将图信号映射到频域后再做卷积操作，将卷积推广到Graph等Non-Euclidean数据上。</p></li><li><p>将图和卷积的 Filter 都做傅立叶变换 Fourier Transform，对二者经过傅立叶变换的结果做 Multiplication，最后再把 Multiplication 的结果做 Inverse Fourier Transform 变回去。</p><p><img src="img/blog/ML-note2-gnn/image-20221119220739396.png" srcset="/img/loading.gif" lazyload alt="image-20221119220739396" style="zoom:50%"></p></li></ul><h3 id="卷积定理">5.2 卷积定理</h3><p>Spectral-based 的方法主要是基于「2.4 图傅里叶变换」。</p><ul><li><p><strong>思想</strong></p><ul><li>Convolution —— Fourier：在适当条件下，<strong>两个信号的卷积的傅立叶变换 = 各自求傅立叶变换转为频域后的点积</strong>，<strong>即对于函数 f 与卷积核 h两者的卷积是其函数傅立叶变换乘积的逆变换。</strong>具体公式如下，</li></ul></li><li><p><strong>图卷积的定义：</strong> <span class="math display">\[ (\boldsymbol{f} * \boldsymbol{h})_{\mathcal{G}}=\boldsymbol{\Phi} \operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right] \boldsymbol{\Phi}^T \boldsymbol{f} \]</span> 其中</p><ul><li>图 <span class="math inline">\(\boldsymbol{f}\)</span> 的傅里叶变换为 <span class="math inline">\(\hat{\boldsymbol{f}}=\boldsymbol{\Phi}^T \boldsymbol{f}\)</span></li><li>卷积核的图傅里叶变换: <span class="math inline">\(\hat{h}=\left(\hat{h}_1, \ldots, \hat{h}_n\right)\)</span> ，按照矩阵形式就是 <span class="math inline">\(\hat{\boldsymbol{h}}=\boldsymbol{\Phi}^T \boldsymbol{h}\)</span></li></ul><p><span class="math display">\[ \hat{h}_k=\left\langle h, \phi_k\right\rangle, k=1,2 \ldots, n \]</span></p><ul><li>求傅里叶变换向量 <span class="math inline">\(\hat{f} \in \mathbb{R}^{N \times 1}\)</span> 和 <span class="math inline">\(\hat{h} \in \mathbb{R}^{N \times 1}\)</span> 的<strong>element-wise乘积</strong>，等价于将 <span class="math inline">\(h\)</span> 组织成 对角矩阵，即 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_k\right)\right] \in \mathbb{R}^{N \times N}\)</span> ，然后再求 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_k\right)\right]\)</span> 和 <span class="math inline">\(\boldsymbol{f}\)</span> 矩阵乘法。</li><li>求上述结果的逆傅里叶变换，即左乘 $ $</li></ul></li><li><p><strong>目的：</strong></p><p>深度学习中的卷积就是要设计 trainable 的卷积核，从公式可以看出，就是要设计 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right]\)</span> 。</p></li></ul><h3 id="gcn-图卷积网络">5.3 GCN | 图卷积网络</h3><p><strong>图卷积</strong>的本质是想找到<strong>适用于图的可学习卷积核</strong>。输入的是整张图，在<code>Convolution Layer 1</code>里，对每个结点的邻居都进行一次卷积操作，并用卷积的结果更新该结点；然后经过激活函数，然后再过一层卷积层<code>Convolution Layer 2</code>与一层激活函数；反复上述过程，直到层数达到预期深度。</p><blockquote><p>GCN vs GNN</p><p>GCN是多层堆叠，上图中的 <code>Layer 1</code> 和 <code>Layer 2</code> 的参数是不同的；GNN是迭代求解，可以看作每一层Layer参数是共享的。</p></blockquote><figure><img src="img/blog/ML-note2-gnn/o_image-10-gcn-framework.png" srcset="/img/loading.gif" lazyload alt="图卷积神经网络全貌"><figcaption>图卷积神经网络全貌</figcaption></figure><ul><li><p><strong>第一代 GCN</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6203">Spectral Networks and Locally Connected Networks on Graphs</a>：可根据「5.2 卷积定理」得到公式</p><p><strong>简单粗暴地把</strong> <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right]\)</span> 转化成直接对卷积核 <span class="math inline">\(\operatorname{diag}\left[\theta_1, \ldots, \theta_n\right]\)</span>学习 ，不需要再将卷积核进行傅里叶变换，直接将变换后的参数进行学习。 <span class="math display">\[ \boldsymbol{y}_{output} = \sigma(\boldsymbol{\Phi} \boldsymbol{g}_{\theta} \boldsymbol{\Phi}^T \boldsymbol{x})=\sigma(\boldsymbol{\Phi} \text{diag}[\theta_1,…,\theta_n] \boldsymbol{\Phi}^T \boldsymbol{x}) \]</span> 其缺陷在于：</p><ul><li>每一次前向传播，都要计算 <span class="math inline">\(\Phi\)</span>，<span class="math inline">\(diag(\theta_n)\)</span> 及 <span class="math inline">\(\Phi^T\)</span> 三者的矩阵乘积，且需要对拉普拉斯矩阵 <span class="math inline">\(L\)</span> 进行谱分解求 <span class="math inline">\(\boldsymbol{\Phi}\)</span>，Graph很大时复杂度较高</li><li>卷积核不具有spatial localization，最终经过多次矩阵相乘，大部分位置元素不为0，是 global 全连接的卷积核。</li><li>卷积核的参数为 <span class="math inline">\(n\)</span></li></ul></li><li><p><strong>第二代 GCN</strong><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a> ：<span class="math inline">\(k\)</span> 阶多项式</p><p>把 <span class="math inline">\(\hat{h}\left(\lambda_l\right)\)</span> 设计为 <span class="math inline">\(\sum_{k=0}^K \theta_k \lambda_l^k\)</span>。<strong>图傅里叶变换</strong>是关于特征值（频率）的函数<span class="math inline">\(F\left(\lambda_1\right), \ldots, F\left(\lambda_n\right)\)</span>, 即 <span class="math inline">\(F(\boldsymbol{\Lambda})\)</span> ，将上述卷积核 <span class="math inline">\(\boldsymbol{g}_\theta\)</span> 写作 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> ，将 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> 定义成如下 <span class="math inline">\(k\)</span> 阶多项式形式： <span class="math display">\[ \boldsymbol{g}_{\boldsymbol{\theta}^{\prime}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k \]</span> 代入可以得到: <span class="math display">\[ \begin{aligned} \boldsymbol{g}_{\boldsymbol{\theta}^{\prime}} * \boldsymbol{x} &amp; \approx \boldsymbol{\Phi} \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T \boldsymbol{x} \\ &amp;=\sum_{k=0}^K \theta_k^{\prime}\left(\mathbf{\Phi} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T\right) \boldsymbol{x} \\ &amp;=\sum_{k=0}^K \theta_k^{\prime}\left(\mathbf{\Phi} \boldsymbol{\Lambda} \boldsymbol{\Phi}^T\right)^k \boldsymbol{x} \\ &amp;=\sum_{k=0}^K \theta_k^{\prime} \boldsymbol{L}^k \boldsymbol{x} \end{aligned} \]</span></p><p>其中 <span class="math inline">\(\theta_0, \theta_1, \ldots,\theta_k\)</span> 是需要学习的参数，通过初始化赋值然后利用误差反向传播进行调整。</p><ul><li>优点在于<ul><li>卷积核只有 <span class="math inline">\(k\)</span> 个参数，一般 <span class="math inline">\(k\)</span> 远小于 <span class="math inline">\(n\)</span>，参数的复杂度被大大降低了。</li><li>该公式无需做特征分解，直接对拉普拉斯矩阵 <span class="math inline">\(L\)</span> 进行 <span class="math inline">\(k\)</span> 次方计算即可，降低时间复杂度</li><li>卷积核具有很好的 <strong>spatial localization</strong>，矩阵 <span class="math inline">\(k\)</span> 次方的物理含义：图的<strong>「 k-hop 连通性」</strong>，元素对应1个节点经过 <span class="math inline">\(k\)</span> 步能否到达另一个顶点，非 0 的话可达，为 0 不可达。在卷积核中， <span class="math inline">\(k\)</span> 就是对应的 receptive field。每次卷积会将中心顶点 K-hop neighbor 上的 feature 进行加权求和，权系数就是 <span class="math inline">\(\theta_k\)</span></li></ul></li><li><strong>ChebNet：</strong>引入<strong>切比雪夫展开式</strong>，切比雪夫多项式是以递归方式定义的一系列正交多项式序列。利用<span class="math inline">\(T_k(\Lambda)\)</span> 的 <span class="math inline">\(k\)</span> 阶截断获得对 <span class="math inline">\(L^k\)</span> 的近似，进而获得对 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> 的近似，来降低时间复杂度。 <span class="math display">\[ \boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{\Lambda}}) \]</span> 切比雪夫多项式 <span class="math inline">\(T_k(\widetilde{\Lambda})\)</span> 使用递归的方式进行定义 <span class="math display">\[ T_0(\widetilde{\Lambda})=\mathrm{I}, T_1(\widetilde{\Lambda})=\widetilde{\Lambda}, T_k(\widetilde{\Lambda})=2 \widetilde{\Lambda} T_{k-1}(\widetilde{\Lambda})-T_{k-2}(\widetilde{\Lambda}) \]</span> where <span class="math inline">\(\tilde{\Lambda}=\frac{2 }{\lambda_{\max }}\Lambda-\mathrm{I}, \quad \tilde{\lambda} \in[-1,1]\)</span></li></ul></li><li><p><strong>第三代GCN</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a> ：进一步简化 ChebNet</p><p>在 ChebNet 的基础上取 <span class="math inline">\(k=1, \lambda_{max}=2,\theta = \theta_0^{\prime}=-\theta_1^{\prime}\)</span>，此时模型是1阶的 first-order proximity。即每层卷积层只考虑了直接邻域，类似CNN中 3*3 的卷积核 <span class="math display">\[ \begin{aligned} \boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} &amp;\approx \theta_0^{\prime} \boldsymbol{x} + \theta_1^{\prime}(\boldsymbol{L}- \boldsymbol{I}_n) \boldsymbol{x} \\ &amp;= \theta_0^{\prime} \boldsymbol{x} - \theta_1^{\prime}(\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}) \boldsymbol{x}\\ &amp;= \theta(\boldsymbol{I_n} + \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}) \boldsymbol{x} \end{aligned} \]</span> 推导中使用了 <strong>归一化的拉普拉斯矩阵</strong> <span class="math display">\[ \boldsymbol{L}=\boldsymbol{D}^{-1/2}(\boldsymbol{D}-\boldsymbol{A})\boldsymbol{D}^{-1/2}=\boldsymbol{I_n}-\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2} \]</span> 此时只有两个参数，即每个卷积核只有2个参数，<span class="math inline">\(\boldsymbol{A}\)</span> 是邻接矩阵。</p><blockquote><p>为什么要归一化？:</p><p>采用加法规则时，对于度大的节点特征越来越大，而对于度小的节点却相反，这可能导致网络训练过程中梯度爆炸或者消失的问题。使用归一化方式，将不再单单地对领域节特征点取平均，它不仅考虑了节点 <span class="math inline">\(i\)</span> 的度，也考虑了邻接节点 <span class="math inline">\(j\)</span> 的度，当邻接节点 <span class="math inline">\(j\)</span> 度数较大时，在聚合时贡献地会更少。</p></blockquote></li></ul><h2 id="benchmark-tasks">6 Benchmark tasks</h2><p>GNN 常应用的任务有以下几类：</p><ul><li><p>classification</p><ul><li>Graph type classification</li><li><p>Edge classification: Traveling Salesman Problem</p></li><li>Semi-supervised node classification<ul><li>pattern recognition</li><li>Semi-supervised graph clustering</li></ul></li></ul></li><li>Regression</li><li>Graph representation learning</li><li><p>Link prediction</p></li></ul><h2 id="references">7 References</h2><ol type="1"><li>蘑菇先生学习记 | 图卷积神经网络理论基础 <a target="_blank" rel="noopener" href="http://xtf615.com/2019/02/24/gcn/" class="uri">http://xtf615.com/2019/02/24/gcn/</a></li><li>SivilTaram | 漫谈图神经网络模型 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html" class="uri">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></li><li>如何理解 Graph Convolutional Network（GCN）？ - superbrother的回答 - 知乎 https://www.zhihu.com/question/54504471/answer/332657604</li><li>如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎 https://www.zhihu.com/question/19714540/answer/1119070975</li></ol></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>ML学习笔记 #02 GNN</div><div>https://yiwen-ding.github.io/ML-note2-gnn</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>CindyWen</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年11月29日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/ML-note1-cnn-rnn" title="ML学习笔记 #01 CNN &amp; RNN"><span class="hidden-mobile">ML学习笔记 #01 CNN &amp; RNN</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"7Ga9gCkoL9V2jdNQgXHdzud4-gzGzoHsz","appKey":"FGdWtMdPhO4fjMsxnDlriMuM","path":"window.location.pathname","placeholder":"欢迎评论:)","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>