<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记 #02 GNN</title>
    <link href="/ML-note2-gnn.html"/>
    <url>/ML-note2-gnn.html</url>
    
    <content type="html"><![CDATA[<h1 id="图神经网络-graph-neural-network">图神经网络 | Graph Neural Network</h1><h2 id="why-gnn">1 Why gnn?</h2><ul><li><p>在实际应用中， 很多数据是图结构的， 比如知识图谱、 社交网络、 分子网络等。但前馈网络和反馈网络很难处理图结构的数据</p></li><li><p>考虑 entity feature 的同时还考虑 entity 之间 的<strong>空间特征</strong>，即 relation 和 structure</p></li><li><p>困难点在于：</p><p>CNN 的<strong>局部平移不变性</strong>，可以很好运用于图片中的规整的二维矩阵 image grids。所谓的「局部平移不变性」即通过卷积操作类似于对<strong>「计算区域内的中心节点和相邻节点进行加权求和」</strong>，无论卷积核平移到图片中的哪个位置都可以保证运算结果的一致性。而图是任意大小的复杂结构，并非是规整的（每个节点的周围邻居数不固定），不能直接使用 CNN。</p><p><img src="img/blog/ML-note2-gnn/o_image-9-cnn-and-gcn.png" alt="GGNN语义解析实例" style="zoom: 67%;"></p></li></ul><h2 id="前置知识-preliminary">2 前置知识 | Preliminary</h2><h3 id="定义-basic-definitions">2.1 定义 | Basic Definitions</h3><p>首先对图中节点、边进行定义，主要包括邻接矩阵和度矩阵。</p><ul><li>Graph: <span class="math inline">\(G=(V, E), N=|V|\)</span></li><li><span class="math inline">\(A \in \mathbb{R}^{N \times N}\)</span>：<strong>「邻接矩阵（Adjacency Matrix）」</strong>, 表示节点之间连接的权重大小，也可以写作 <span class="math inline">\(W\)</span></li></ul><p><span class="math display">\[A_{i, j}=0 \text { if } e_{i, j} \notin E \text {, else } A_{i, j}=w(i, j)\]</span></p><ul><li><span class="math inline">\(D \in \mathbb{R}^{N \times N}\)</span>: <strong>「度矩阵（Degree Matrix）」</strong>，为对角矩阵，表示当前节点的度数之和</li></ul><p><span class="math display">\[D_{i, j}=\left\{\begin{array}{ll}d(i) &amp; \text { if } i=j \quad \text { (Sum of row } i \text { in } A) \\0 &amp; \text { if } i \neq j\end{array} \quad\right.\]</span></p><ul><li><p><span class="math inline">\(f: V \rightarrow \mathbb{R}^N\)</span>：<strong>「图信号（signal on graph - vertex）」</strong> <span class="math inline">\(f(i)\)</span> denotes the signal on vertex <span class="math inline">\(i\)</span></p><figure><img src="img/blog/ML-note2-gnn/image-20221121194423741.png" alt="image-20221121194423741"><figcaption>image-20221121194423741</figcaption></figure></li><li><p><span class="math inline">\(L=D-A, L \geqslant 0\)</span>：<strong>「拉普拉斯矩阵（Laplacian Matrix）」</strong>是<strong>半正定对称矩阵</strong></p><ul><li><span class="math inline">\(L\)</span> 是对称的，实对称矩阵一定可以用<strong>正交矩阵</strong>进行正交相似对角化： <span class="math inline">\(L=\Phi \Lambda \Phi^{-1}=\Phi \Lambda \Phi^{\mathrm{T}}\)</span></li><li>矩阵特征值<span class="math inline">\(\Lambda=\operatorname{diag}\left(\lambda_0, \ldots, \lambda_{N-1}\right) \in \mathbb{R}^{N \times N}\)</span><strong>一定非负</strong></li><li>根据实对称矩阵<strong>不同特征值</strong>所对应的<strong>特征向量必定正交</strong>。 <span class="math inline">\(\Phi=\left[\phi_0, \ldots, \phi_{N-1}\right] \in \mathbb{R}^{N \times N}\)</span>是正交矩阵 <span class="math inline">\(\lambda_l\)</span> 代表频率， <span class="math inline">\(\phi_l\)</span> 代表 <span class="math inline">\(\lambda_l\)</span> 对应的基向量</li></ul><p><img src="img/blog/ML-note2-gnn/image-20221121202032720.png" alt="image-20221121202032720" style="zoom:50%;"></p><ul><li>通过图信号非0向量 <span class="math inline">\(f\)</span> 计算可得到光滑程度。根据「若对于每个非零实向量<span class="math inline">\(X\)</span>，都有 <span class="math inline">\(X'AX\ge0\)</span>，则称A为<strong>半正定矩阵</strong>」的定理，可知矩阵 <span class="math inline">\(L\)</span> 是<strong>半正定</strong>的。</li></ul><p><span class="math display">\[\begin{aligned}f^{T}Lf &amp;=f^{T}(Df - Af) \\&amp;=\sum_{v_i \in V} f\left(v_i\right) \sum_{v_j \in V} w_{i, j}\left(f\left(v_i\right)-f\left(v_j\right)\right) \\&amp;=\sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f^2\left(v_i\right)-f\left(v_i\right) f\left(v_j\right)\right) \\&amp;=\frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f^2\left(v_i\right)-f\left(v_i\right) f\left(v_j\right)+f^2(j)-f\left(v_j\right) f\left(v_i\right)\right) \\&amp;=\frac{1}{2} \sum_{v_i \in V} \sum_{v_j \in V} w_{i, j}\left(f\left(v_i\right)-f\left(v_j\right)\right)^2 \ge0 \quad \begin{array}{l}\text { "Power" of signal variation } \\\text { between nodes, i.e., } \\\text { smoothness of graph signal }\end{array}\end{aligned}\]</span></p></li></ul><h3 id="拉普拉斯算子-laplacian">2.2 拉普拉斯算子 | Laplacian</h3><p>图拉普拉斯矩阵<span class="math inline">\(L\)</span> 为什么要这样定义的？首先得了解什么是拉普拉斯算子。</p><p>在数学中，拉普拉斯算子 (Laplacian) 是由欧几里得空间中的一个函数的梯度的<strong>散度</strong>给出的<strong>微分算子</strong>，通常有以下几种写法: <span class="math inline">\(\Delta, \nabla^2, \nabla \cdot \nabla\)</span> 。所以对于任意函数 <span class="math inline">\(f\)</span> 来说，其拉普拉斯算子的定义为: <span class="math display">\[\Delta f=\nabla^2 f=\nabla \cdot \nabla f\\=\sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2}\]</span></p><p>以一维空间为例： <span class="math display">\[\begin{aligned}\frac{\partial^2 f}{\partial x_i^2} &amp;=f^{\prime \prime}(x) \\&amp; \approx f^{\prime}(x)-f^{\prime}(x-1) \\&amp; \approx f(x+1)-f(x)-(f(x)-f(x-1)) \\&amp;=f(x+1)+f(x-1)-2 f(x)\end{aligned}\]</span> 也就是说二阶导数近似于其二阶差分，可以理解为当前点对其在所有自由度上微扰之后获得的增益。这里自由度为 2，分别是 +1 和 -1 方向。</p><p>再以二维空间为例子：此时共有 4 个自由度 (1,0),(-1,0),(0,1),(0,-1)，当然如果对角线后其自由度可以为 8。 <span class="math display">\[\begin{aligned}\Delta f(x, y) &amp;=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2} \\&amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\&amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y)\end{aligned}\]</span> 归纳可得，<strong>「拉普拉斯算子是所有自由度上进行微小变化后所获得的增益」</strong>。</p><p>推广到网络图中，考虑有 N 个节点的网络图，其自由度最大为 N，那么函数 <span class="math inline">\(f\)</span> 可以是 N 维的向量，即： <span class="math display">\[f= (f_1,...f_N)\]</span> 其中 <span class="math inline">\(f_i\)</span> 表示在网络图中节点 <span class="math inline">\(i\)</span> 处的函数值。</p><p>在网络图中，两个节点的之间的增益为 <span class="math inline">\(f_i-f_j\)</span> ，考虑加权图则有 <span class="math inline">\(w_{i j}\left(f_i-f_j\right)\)</span> ，那么对于节点 <span class="math inline">\(i\)</span> 来说，总增益即为拉普拉斯算子在节点 <span class="math inline">\(i\)</span> 的值: <span class="math display">\[\begin{aligned}\Delta \boldsymbol{f}_i &amp;=\sum_{j \in N_i} \frac{\partial f_i}{\partial j^2} \\&amp; \approx \sum_j w_{i j}\left(f_i-f_j\right) \\&amp;=\sum_j w_{i j}\left(f_i-f_j\right) \\&amp;=\left(\sum_j w_{i j}\right) f_i-\sum_j w_{i j} f_j \\&amp;=d_i f_i-w_{i:} f_i\end{aligned}\]</span> 其中， <span class="math inline">\(d_i=\sum_{j \in N_i} w_{i j}\)</span> 为节点 <span class="math inline">\(i\)</span> 的度；上式第二行去掉了 <span class="math inline">\(j \in N_i\)</span> 是因为 <span class="math inline">\(w_{i j}\)</span> 可以控制节点 <span class="math inline">\(i\)</span> 的邻接矩阵。 对于任意 <span class="math inline">\(i \in N\)</span> 都成立，所以有: <span class="math display">\[\begin{aligned}\Delta f=\left(\begin{array}{c}\Delta f_1 \\\vdots \\\Delta f_N\end{array}\right) &amp;=\left(\begin{array}{cc}d_1 f_1-w_{1:} f \\\vdots \\d_N f_N-w_{N:} f\end{array}\right) \\&amp;=\left(\begin{array}{ccc}d_1 &amp; \cdots &amp; 0 \\\vdots &amp; \ddots &amp; \vdots \\0 &amp; \cdots &amp; d_N\end{array}\right) f-\left(\begin{array}{c}w_{1:} \\\vdots \\w_{N:}\end{array}\right) f \\&amp;=\operatorname{diag}\left(d_i\right) f-\mathbf{W} f \\&amp;=(\mathbf{D}-\mathbf{W}) f \\&amp;=\mathbf{L} f\end{aligned}\]</span> 这个公式的全称为：图拉普拉斯算子作用于 <span class="math inline">\(f\)</span> 上的结果等于「<strong>图拉普拉斯矩阵和向量 <span class="math inline">\(f\)</span> 的点积</strong>」。<span class="math inline">\(L\)</span> 也直接称作图拉普拉斯算子，反映了当前节点<strong>对周围节点</strong>产生扰动时所产生的累积增益，直观上也可以理解为某一节点的权值变为其相邻节点权值的期望影响，可以刻画局部的<strong>平滑度</strong>。</p><h3 id="傅里叶级数-fourier-series">2.3 傅里叶级数 | Fourier Series</h3><ul><li>定义：参考用函数的<strong>「幂级数展开式」</strong>表示与讨论函数的思想，傅里叶级数尝试将<strong>非正弦周期</strong>函数展开成三角函数组成的级数。具体地说，将周期为 <span class="math inline">\(T\left(=\frac{2 \pi}{\omega}\right)\)</span> 的周期函数用一系列以 <span class="math inline">\(T\)</span> 为周期的正弦函数 <span class="math inline">\(A_n \sin \left(n \omega t+\varphi_n\right)\)</span> 组成的级数来表示，记为</li></ul><p><span class="math display">\[f(t)=A_0+\sum_{n=1}^{\infty} A_n \sin \left(n \omega t+\varphi_n\right),\]</span></p><p>其中 <span class="math inline">\(A_0, A_n, \varphi_n(n=1,2,3, \cdots)\)</span> 都是常数。将其中的 sin 正弦函数展开，并且令 <span class="math inline">\(\frac{a_0}{2}=A_0,a_n=A_n \sin \varphi_n,\)</span> <span class="math inline">\(b_n=A_n \cos \varphi_n, \omega=\frac{\pi}{l}(\)</span> 即 <span class="math inline">\(T=2 l)\)</span>，式子可改写为 <span class="math display">\[\begin{aligned}f(t)&amp;=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left(a_n \cos \frac{n \pi t}{l}+b_n \sin \frac{n \pi t}{l}\right)\\&amp;=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left(a_n \cos n x+b_n \sin n x\right) \;given \;\frac{\pi t}{l}=x\end{aligned}\]</span></p><ul><li><p>物理含义：将一个复杂的周期运动看成是许多<strong>不同频率</strong>的简谐振动的<strong>叠加</strong>，其中不同的振荡函数具有不同的振幅和频率。</p><p>以 <span class="math inline">\(f(t) \approx 2.5+\frac{10}{\pi}\left(\sin \frac{\pi t}{4}+\frac{1}{3} \sin \frac{3 \pi t}{4}+\frac{1}{5} \sin \frac{5 \pi t}{4}+\frac{1}{7} \sin \frac{7 \pi t}{4}\right)\)</span> 为例，考虑以频率为横坐标，振幅为纵坐标，可绘制频域函数图如下。</p><p><img src="img/blog/ML-note2-gnn/v2-0b06eb050563bdfdd8bdde88e98757c1_r.jpg" alt="img" style="zoom: 80%;"></p><p>从下图可感受<strong>时域信息到频域信息的转化</strong>。</p><ul><li>时域：时间和振幅的关系图，横坐标是<strong>时间</strong>，纵坐标是振幅。</li><li>频域：频率和振幅的关系图，横坐标是<strong>频率</strong>，纵坐标是振幅。</li></ul><p><img src="img/blog/ML-note2-gnn/v2-dace79414a915197e605b7b1fde36ba0_720w.webp" alt="img" style="zoom: 67%;"></p><figure><embed src="img/blog/ML-note2-gnn/v2-38d2e076c57f6d3dd28749c77c89ed6e_720w.webp"><figcaption>img</figcaption></figure></li><li><p>三角函数系：</p><p>正交基满足 <span class="math display">\[\begin{aligned}  &amp;\begin{cases}  \int_{-\pi}^{\pi}{\cos}(mx)\cos\mathrm{(}nx)dx=\pi ,&amp;       m=n,m,n\ge 1\\  \int_{-\pi}^{\pi}{\cos}(mx)\cos\mathrm{(}nx)dx=0,&amp;      m\ne n,m,n\ge 1\\\end{cases}\\  &amp;\begin{cases}  \int_{-\pi}^{\pi}{\sin}(mx)\sin\mathrm{(}nx)dx=\pi ,&amp;       m=n,m,n\ge 1\\  \int_{-\pi}^{\pi}{\sin}(mx)\sin\mathrm{(}nx)dx=0,&amp;      m\ne n,m,n\ge 1\\\end{cases}\\  &amp;\int_{-\pi}^{\pi}{\cos}(mx)\sin\mathrm{(}nx)dx=0,\quad m,n\ge 1\\  &amp;\int_{-\pi}^{\pi}{\cos}(nx)dx=0,\quad n\ge 1\\  &amp;\int_{-\pi}^{\pi}{\sin}(nx)dx=0,\quad n\ge 1\\\end{aligned}\]</span></p></li></ul><p>​ 因此，<span class="math inline">\(1,\;cos x,\;sin x,\;cos 2x,\;sin 2x,\;cos nx,\;sin nx\)</span> 构成一组<strong>标准正交基</strong>。任何不同的两个函数的乘积在区间 <span class="math inline">\([-T，T]\)</span> 上的积分等于 0。</p><ul><li><p>系数求解：</p><p>通过将右端的级数逐项积分，等式两端同时乘以对应三角函数，利用正交性抵消，求解系数 <span class="math inline">\(a_0, a_1, b_1\cdots\)</span> ，称其为<strong>「傅里叶级数」</strong>。</p></li></ul><h3 id="傅里叶变换-fourier-transformer">2.4 傅里叶变换 | Fourier Transformer</h3><p>前述的傅里叶级数适用于<strong>非正弦周期</strong>函数的变换，但现实中大部分函数都是非周期的，接下来讨论涉及到非周期性函数的方法。</p><p>在介绍非周期性函数之前，首先介绍下<strong>欧拉公式</strong>。</p><p>考虑横轴为 1，纵轴为虚单位 i 的坐标系，图上任意一点都可以表示为 <span class="math inline">\(cos\theta + isin\theta\)</span>，根据欧拉公式，可以写成： <span class="math inline">\(cos\theta + isin\theta = e^{i\theta}\)</span>，其含义为<strong>找到权重为正余弦的两个正交基（实部和虚部）</strong></p><p><img src="img/blog/ML-note2-gnn/euler.png" alt="euler" style="zoom: 60%;"></p><p>以时间 t 为横坐标，则可以记录到坐标点 A 映射在虚轴的运动轨迹，图中体现了频域和时域的相互转化。</p><p><img src="img/blog/ML-note2-gnn/iwt.png" alt="iwt" style="zoom: 67%;"></p><p>回到非周期函数的傅立叶变换的讨论，可以将非周期函数考虑为周期无穷大的函数，考虑频域中的横坐标：<span class="math inline">\(f=\frac{1}{T}\)</span>，当周期 T 无穷大时，频域图就从离散点变为连续的曲线，如下图所示。</p><p><img src="img/blog/ML-note2-gnn/fourier_transform_jishu.png" alt="fourier_transform_jishu" style="zoom: 67%;"></p><ul><li><strong>复数形式</strong>：傅里叶级数展开公式中有正弦波，也有余弦波，画频域图也不方便，<strong>通过欧拉公式，可以修改为复数形式</strong>：</li></ul><p><span class="math display">\[f(x)=\sum_{n=-\infty}^{\infty} c_n \cdot e^{i \frac{2 \pi n}{T} x}\]</span></p><ul><li><strong>逆傅里叶变换公式</strong> 如下：复数形式也可以理解为向量， <span class="math inline">\(e^{i \frac{2 \pi n}{T} x}\)</span> 是基， <span class="math inline">\(c_n\)</span> 是该基下的坐标。从数学角度， <span class="math inline">\(f(x)\)</span> 是函数 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(x\)</span> 处的取值，所有基都对该处取值有贡献，即把每个 <span class="math inline">\(F(w)\)</span> 投影 到 <span class="math inline">\(e^{i w x}\)</span> 基方向上分量累加起来，得到的就是该点处的函数值。</li></ul><p><span class="math display">\[f(x)=\int_{-\infty}^{\infty} F(w) e^{i w x} d w=\sum_w F(w) e^{i w x}\]</span></p><ul><li><strong>傅立叶变换</strong> (Fourier Transform，FT) 公式如下，其中， <span class="math inline">\(e^{-i w t}\)</span> 是一组正交基的组合。「实数部分表示振幅」，「虚数部分表示相位」。从数学角度， <span class="math inline">\(F(w)\)</span> 就是每个基下对应的坐标值，所有的 <span class="math inline">\(x\)</span> 对该基都有贡献，即把每个<span class="math inline">\(f(x)\)</span> 投影到 <span class="math inline">\(e^{-iwx}\)</span> 基方向上的分量全部累加起来，得到的就是该基方向的坐标值。<strong>本质上是将函数 <span class="math inline">\(f(x)\)</span> 映射到了以 <span class="math inline">\(e^{-iwx}\)</span> 为基向量的空间中。</strong></li></ul><p><span class="math display">\[F(w)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} f(x) e^{-iwx} d x\]</span></p><ul><li><strong>相互转换：</strong></li></ul><p><span class="math display">\[f(x)⇔F(w)\]</span></p><p>一个是函数，一个是<strong>向量</strong>（频域曲线纵坐标构成的向量，基为频域曲线横坐标对应的的基函数）。</p><h3 id="图的傅里叶变换">2.5 图的傅里叶变换</h3><ul><li>传统的傅里叶变换</li></ul><p><span class="math display">\[F(k)=\frac{1}{2 \pi} \int_{-\infty}^{\infty} f(x) e^{-i k x} d x \approx\left\langle f, e^{-i k x}\right\rangle\]</span></p><p><span class="math inline">\(F(k)\)</span> 是傅里叶系数（即频率为 <span class="math inline">\(k\)</span> 时的振幅值）。约等号去掉了常数系数，同时 <span class="math inline">\(x\)</span> 为离散变量 时，离散积分等于内积。<span class="math inline">\(e^{-i k x}\)</span> 为 Fourier Basis。可以证明 <span class="math inline">\(e^{-i k x}\)</span> 是<strong>拉普拉斯算子的特征函数</strong>（满足特征方程 <span class="math inline">\(A V=\lambda V)\)</span> ，证明: <span class="math display">\[\Delta e^{-i k x}=\frac{\partial e^{-i k x}}{\partial x^2}=-k^2 e^{-i k x}\]</span></p><ul><li>在Graph上作类比，设 <span class="math inline">\(\phi_k\)</span> 是图拉普拉斯算子 <span class="math inline">\(\boldsymbol{L}\)</span> 的特征向量，（满足 <span class="math inline">\(\boldsymbol{L} \phi_k=\lambda_k \phi_k\)</span> ）。在 Graph中， <span class="math inline">\(\Delta=\boldsymbol{L}\)</span>, <span class="math inline">\(e^{-i k x}=\phi_{\boldsymbol{k}}\)</span>。因此，为了在Graph上进行傅里叶变换，可以把传统傅里叶变换中的基 <span class="math inline">\(e^{-i k x}\)</span> 换成 <span class="math inline">\(\phi_{\boldsymbol{k}}\)</span> （是<strong>线性无关的正交向量</strong>）。 <span class="math display">\[F(\lambda_k)=\left\langle f, \phi_\boldsymbol{k}\right\rangle\\\]</span> 矩阵形式的<strong>图傅里叶变换</strong>： <span class="math display">\[\hat{\boldsymbol{f}} = \boldsymbol{\Phi}^T \boldsymbol{f}\]</span> <strong>迁移到Graph上的逆傅里叶变换</strong> <span class="math display">\[f_i= \sum_{k=1}^n \hat{f}_k (\boldsymbol{\phi_{k}}^T )_i\]</span> 推广到矩阵形式， <span class="math display">\[\boldsymbol{f} = \boldsymbol{\Phi} \boldsymbol{\hat{f}}\]</span></li></ul><table><thead><tr class="header"><th style="text-align: left;">传统傅里叶变换</th><th style="text-align: left;">图傅里叶变换</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">频率（<span class="math inline">\(k\)</span>）</td><td style="text-align: left;">特征值（<span class="math inline">\(\lambda_k\)</span>）</td></tr><tr class="even"><td style="text-align: left;">正弦函数 <span class="math inline">\(e^{-i k x}\)</span></td><td style="text-align: left;">特征向量 <span class="math inline">\(\phi_\boldsymbol{k}\)</span></td></tr><tr class="odd"><td style="text-align: left;">振幅 <span class="math inline">\(F(k)\)</span></td><td style="text-align: left;">振幅 <span class="math inline">\(F(\lambda_k)\)</span></td></tr></tbody></table><h2 id="gnn-模型架构-model">3 GNN 模型架构 | Model</h2><p>GNN 理论基础是<strong>不动点</strong>理论，被划分成 <strong>空域（Spatial-based GNN）</strong> 和 <strong>谱域（Spectral-based GNN）</strong> 两大类，空域模型不需要矩阵特征分解，能直接在空域视角进行矩阵计算；谱域模型则从信号处理的角度实现 GNN。</p><h3 id="状态更新与输出">3.1 <strong>状态更新与输出</strong></h3><p>在一个图结构中，<strong>每一个节点由它自身的特征以及与其相连的节点特征来定义该节点</strong>。GNN的目标是学习得到每个结点的图感知的隐藏状态 <span class="math inline">\(\mathbf{h}_v\)</span> (state embedding)</p><p>GNN通过迭代式更新所有结点的隐藏状态来实现，在 <span class="math inline">\(t+1\)</span> 时刻，结点 <span class="math inline">\(v\)</span> 的隐藏状态按照如下 方式更新: <span class="math display">\[\begin{aligned}\mathbf{h}_v^{t+1}&amp;=f\left(\mathbf{x}_v, \mathbf{x}_c o[v], \mathbf{h}_n^t e[v], \mathbf{x}_n e[v]\right)\\o_v &amp;= g(h_v,x_v)\end{aligned}\]</span> 用 <span class="math inline">\(\mathbf{x}_v\)</span> 表示结点 <span class="math inline">\(\mathrm{V}\)</span> 的特征；连接两个结点的边也有自己的特征， <span class="math inline">\(\mathbf{x}_{(v, u)}\)</span> 表示结点 <span class="math inline">\(\mathrm{v}\)</span> 与结点 <span class="math inline">\(\mathrm{u}\)</span> 之间边的特征。</p><ul><li><span class="math inline">\(f\)</span> 就是隐藏状态的状态更新函数，也被称为<strong>局部转移函数</strong> (local transaction function)<strong>，这个函数在所有节点中共享，带有参数</strong>。</li><li>公式中的 <span class="math inline">\(\mathbf{x}_c o[v]\)</span> 指 的是与结点 <span class="math inline">\(v\)</span> 相邻的边的特征</li><li><span class="math inline">\(\mathbf{x}_n e[v]\)</span> 指的是结点 <span class="math inline">\(v\)</span> 的邻居结点的特征</li><li><span class="math inline">\(\mathbf{h}_n^t e[v]\)</span> 则指邻居结点在 <span class="math inline">\(t\)</span> 时刻的隐藏状态。</li><li><span class="math inline">\(g\)</span> 又被称为<strong>局部输出函数</strong>(local output function)，用于描述输出的产生方式。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-2-state-update-function.png" alt="更新公式示例" style="zoom: 33%;"></p><blockquote><p>更新公式的含义：不断地利用当前时刻邻居结点的隐藏状态作为部分输入来生成下一时刻中心结点的隐藏状态，直到每个结点的隐藏状态变化幅度很小，整个图的信息流动趋于平稳。至此，每个结点都“知晓”了其邻居的信息。</p></blockquote><p>更新至收敛的条件：通过两个时刻 <span class="math inline">\(p\)</span>范数的差值是否小于某个阈值 <span class="math inline">\(\epsilon\)</span> 来判定的 <span class="math display">\[||H^{t+1}||_2−||H^t||_2&lt;\epsilon\]</span></p><p>用 <span class="math inline">\(F\)</span> 表示若干个 <span class="math inline">\(f\)</span> <strong>堆叠</strong>得到的一个函数，也称为<strong>全局更新</strong>函数，那么图上所有结点的状态更新公式可以写成： <span class="math display">\[H^{t+1}=F(H^t,X)\]</span></p><ul><li><strong>不动点</strong>(the fixed point)理论指的是：只要 <span class="math inline">\(F\)</span> 是个<strong>压缩映射</strong>(contraction map)， <span class="math inline">\(H^0\)</span> 经过不断迭代都会收敛到某一个固定的点，称之为不动点。</li><li><strong>压缩映射 </strong>的含义：任意两个点 <span class="math inline">\(x, y\)</span> 在经过 <span class="math inline">\(F\)</span> 映射后变为 <span class="math inline">\(F(x), F(y)\)</span> 。 <span class="math inline">\(d(F(x), F(y)) \leq c d(x, y), 0 \leq c&lt;1\)</span> 。 经过 <span class="math inline">\(F\)</span> 变换后的新空间一定比原先的空间要小，原先的空间被压缩了。想象这种压缩的过程不断进行，最终就会把原空间中的所有点映射到一个点上。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-3-contraction-map.png" alt="更新公式示例" style="zoom: 50%;"></p><h3 id="模型学习">3.2 模型学习</h3><p>通过模型学习，让 <span class="math inline">\(f\)</span> 接近压缩映射。使用目标信息（<span class="math inline">\(t_v\)</span>表示特定节点的标签）来进行监督学习，loss定义如下： <span class="math display">\[loss=\sum_{i = 1}^{p}(t_i - o_i)\]</span> 根据 <strong>前向传播计算损失、反向传播计算梯度</strong> 进行学习</p><ul><li>状态 <span class="math inline">\(h_v^t\)</span> 按照迭代方程更新 <span class="math inline">\(T\)</span> 个轮次，直至收敛。</li><li>对于有监督信号的结点，将其隐藏状态通过 <span class="math inline">\(g\)</span> 得到输出，进而算出模型的损失。</li><li>得到的 <span class="math inline">\(H\)</span> 会接近不动点的解 $H(T)≈H $。</li><li>反向传播计算 <span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span> 对 <span class="math inline">\(h_v^0\)</span> 的梯度，用于更新模型的参数。</li></ul><h3 id="gnn-vs-rnn">3.3 GNN vs RNN</h3><p>将循环神经网络与图神经网络对比，存在以下不同点：</p><p>假设在GNN中存在三个结点 <span class="math inline">\(x_1, x_2, x_3\)</span>，在RNN中有一个序列 <span class="math inline">\((x_1, x_2, x_3)\)</span>。</p><ul><li><strong>输入：</strong>GNN每次时间步的输入都是<strong>所有结点</strong> <span class="math inline">\(v\)</span> 的特征，而 RNN 每次时间步的输入是<strong>该时刻</strong>的输入。同时，时间步之间的信息流也不相同，前者由边决定，后者则由序列的读入顺序决定。</li><li><strong>训练过程：</strong>GNN的基础理论是不动点理论，因此其沿时间展开的长度是动态的，<strong>根据收敛条件</strong>确定的，而RNN沿时间展开的长度就等于<strong>序列本身的长度</strong>。</li><li><strong>训练目标：</strong>GNN目标是得到每个结点<strong>稳定的隐藏状态</strong>，只有在隐藏状态收敛后才能输出；而RNN的每个时间步上都可以输出，比如语言模型。</li></ul><p><img src="img/blog/ML-note2-gnn/o_image-5-gnn-rnn.png" alt="GNN与RNN的区别" style="zoom: 50%;"></p><h3 id="局限性">3.4 局限性</h3><p>原始 GNN 的核心观点是<strong>通过结点信息的传播使整张图达到收敛，在其基础上再进行预测</strong>。问题在于：</p><ul><li>GNN只将边作为一种传播手段，但并未区分不同边的功能。虽然可以在特征构造阶段 <span class="math inline">\((x_{(u,v)})\)</span> 为不同类型的边赋予不同的特征，但相比于其他输入，边对结点隐藏状态的影响有限。并且GNN没有为边设置独立的可学习参数，无法通过模型学习到边的某些特性。</li><li>GNN应用在 <strong>节点表示</strong> 的场景中，使用不动点理论并不合适。因为基于不动点的收敛会导致结点之间的隐藏状态间存在较多信息共享，<strong>不动点的向量表示分布在数值上会非常的平滑</strong>，属于结点自身的特征<strong>信息匮乏</strong>。</li></ul><h2 id="spatial-based-gnn">4 Spatial-based GNN</h2><p>由于传统的卷积不能直接用在图上，主要的难点在于<strong>邻居结点数量不固定</strong>。</p><ul><li><p>思想：</p><ul><li><p>aggregate: 用 neighbor feature 更新下一层的 hidden state</p></li><li><p>readout: 将所有 nodes 的 feature 集合起来代表整个 graph</p><p><img src="img/blog/ML-note2-gnn/image-20221114225603657.png" alt="image-20221114225603657" style="zoom:67%;"></p></li></ul></li><li><p>具体的聚合方式</p></li></ul><table><thead><tr class="header"><th>Aggregation</th><th>Method</th></tr></thead><tbody><tr class="odd"><td>Sum</td><td>NN4G</td></tr><tr class="even"><td>Mean</td><td>DCNN, DGC, GraphSAGE</td></tr><tr class="odd"><td>Weighted sum</td><td>MoNET, GAT, GIN</td></tr><tr class="even"><td>LSTM</td><td>GraphSAGE</td></tr><tr class="odd"><td>Max Pooling</td><td>GraphSAGE</td></tr></tbody></table><h3 id="消息传递网络-mpnn">4.1 消息传递网络 | MPNN</h3><p>消息传递网络（Message Passing Neural Network）将空域卷积分解为两个过程：<strong>消息传递</strong>与<strong>状态更新</strong>操作，分别由$M_l(⋅) $ 和 <span class="math inline">\(U_l(⋅)\)</span> 函数完成。将结点 <span class="math inline">\(v\)</span> 的特征 <span class="math inline">\(x_v\)</span> 作为其隐藏状态的初始态 <span class="math inline">\(h_0^v\)</span> 后，空域卷积对隐藏状态的更新由如下公式表示： <span class="math display">\[\mathbf{h}_{v}^{l+1}=U_{l+1}(\mathbf{h}_v,\sum_{u{\in}ne[v]}M_{l+1}(\mathbf{h}_v^l,\mathbf{h}_u^l,\mathbf{x}_{vu}))\]</span> 其中 <span class="math inline">\(l\)</span> 代表图卷积的第 <span class="math inline">\(l\)</span> 层，上式的物理意义是：收到来自每个邻居的的消息 <span class="math inline">\(M_{l+1}\)</span> 后，每个结点如何更新自己的状态。</p><h3 id="graphsage">4.2 GraphSAGE</h3><p><a href="https://arxiv.org/pdf/1706.02216.pdf">GraphSAGE</a> 全称为 Graph Sample and Aggregate。区别于传统的全图卷积（将所有结点放入内存/显存中），GraphSAGE 利用<strong>采样</strong> (Sample) 部分结点的方式进行学习。当然，即使不需要整张图同时卷积，GraphSage仍然需要聚合邻居结点的信息，即 <em>aggregate</em> 的操作。这种操作类似于 MPNN 中的<strong>消息传递</strong>过程。</p><p><img src="img/blog/ML-note2-gnn/image-20221124171720451.png" alt="image-20221124171720451" style="zoom: 67%;"></p><ul><li><p>采样过程</p><ul><li>在图中随机<strong>采样</strong>若干个结点，结点数为 <code>batch_size</code>。对于每个结点，随机选择固定数目的邻居结点（一阶邻居，或是二阶邻居）构成进行卷积操作的图。</li><li>将邻居结点的信息通过 aggregate 函数聚合起来更新刚才采样的结点。</li><li>计算采样结点处的损失。如果是无监督任务，希望图上邻居结点的编码相似；如果是监督任务，可根据具体结点的任务标签计算损失。</li></ul></li><li><p>更新公式为 <span class="math display">\[\mathbf{h}_{v}^{l+1}=\sigma(\mathbf{W}^{l+1}\cdot aggregate(\mathbf{h}_v^l,\{\mathbf{h}_u^l\}),{\forall}u{\in}ne[v])\]</span></p></li><li><p>聚类器</p><p>GraphSAGE 的设计重点就放在了 aggregate 函数的设计上。它可以是不带参数的max，mean，也可以是带参数的如 LSTM 的等神经网络。核心的原则是需要可以处理变长的数据。</p></li><li><p>GraphSAGE 适用于 Inductive learning，扩展到新的节点和新的图，因为直接学习的采样过程和加权求和方式，并没有利用图的拉普拉斯矩阵。</p></li></ul><h2 id="spectral-based-gnn">5 Spectral-based GNN</h2><h3 id="动机">5.1 动机</h3><ul><li><p><strong>动机：</strong>既然无法直接在时域进行卷积，就将图信号映射到频域后再做卷积操作，将卷积推广到Graph等Non-Euclidean数据上。</p></li><li><p>将图和卷积的 Filter 都做傅立叶变换 Fourier Transform，对二者经过傅立叶变换的结果做 Multiplication，最后再把 Multiplication 的结果做 Inverse Fourier Transform 变回去。</p><p><img src="img/blog/ML-note2-gnn/image-20221119220739396.png" alt="image-20221119220739396" style="zoom:50%;"></p></li></ul><h3 id="卷积定理">5.2 卷积定理</h3><p>Spectral-based 的方法主要是基于「2.4 图傅里叶变换」。</p><ul><li><p><strong>思想</strong></p><ul><li>Convolution —— Fourier：在适当条件下，<strong>两个信号的卷积的傅立叶变换 = 各自求傅立叶变换转为频域后的点积</strong>，<strong>即对于函数 f 与卷积核 h两者的卷积是其函数傅立叶变换乘积的逆变换。</strong>具体公式如下，</li></ul></li><li><p><strong>图卷积的定义：</strong> <span class="math display">\[(\boldsymbol{f} * \boldsymbol{h})_{\mathcal{G}}=\boldsymbol{\Phi} \operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right] \boldsymbol{\Phi}^T \boldsymbol{f}\]</span> 其中</p><ul><li>图 <span class="math inline">\(\boldsymbol{f}\)</span> 的傅里叶变换为 <span class="math inline">\(\hat{\boldsymbol{f}}=\boldsymbol{\Phi}^T \boldsymbol{f}\)</span></li><li>卷积核的图傅里叶变换: <span class="math inline">\(\hat{h}=\left(\hat{h}_1, \ldots, \hat{h}_n\right)\)</span> ，按照矩阵形式就是 <span class="math inline">\(\hat{\boldsymbol{h}}=\boldsymbol{\Phi}^T \boldsymbol{h}\)</span></li></ul><p><span class="math display">\[\hat{h}_k=\left\langle h, \phi_k\right\rangle, k=1,2 \ldots, n\]</span></p><ul><li>求傅里叶变换向量 <span class="math inline">\(\hat{f} \in \mathbb{R}^{N \times 1}\)</span> 和 <span class="math inline">\(\hat{h} \in \mathbb{R}^{N \times 1}\)</span> 的<strong>element-wise乘积</strong>，等价于将 <span class="math inline">\(h\)</span> 组织成 对角矩阵，即 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_k\right)\right] \in \mathbb{R}^{N \times N}\)</span> ，然后再求 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_k\right)\right]\)</span> 和 <span class="math inline">\(\boldsymbol{f}\)</span> 矩阵乘法。</li><li>求上述结果的逆傅里叶变换，即左乘 $ $</li></ul></li><li><p><strong>目的：</strong></p><p>深度学习中的卷积就是要设计 trainable 的卷积核，从公式可以看出，就是要设计 <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right]\)</span> 。</p></li></ul><h3 id="gcn-图卷积网络">5.3 GCN | 图卷积网络</h3><p><strong>图卷积</strong>的本质是想找到<strong>适用于图的可学习卷积核</strong>。输入的是整张图，在<code>Convolution Layer 1</code>里，对每个结点的邻居都进行一次卷积操作，并用卷积的结果更新该结点；然后经过激活函数，然后再过一层卷积层<code>Convolution Layer 2</code>与一层激活函数；反复上述过程，直到层数达到预期深度。</p><blockquote><p>GCN vs GNN</p><p>GCN是多层堆叠，上图中的 <code>Layer 1</code> 和 <code>Layer 2</code> 的参数是不同的；GNN是迭代求解，可以看作每一层Layer参数是共享的。</p></blockquote><figure><img src="img/blog/ML-note2-gnn/o_image-10-gcn-framework.png" alt="图卷积神经网络全貌"><figcaption>图卷积神经网络全貌</figcaption></figure><ul><li><p><strong>第一代 GCN</strong> <a href="https://arxiv.org/abs/1312.6203">Spectral Networks and Locally Connected Networks on Graphs</a>：可根据「5.2 卷积定理」得到公式</p><p><strong>简单粗暴地把</strong> <span class="math inline">\(\operatorname{diag}\left[\hat{h}\left(\lambda_1\right), \ldots, \hat{h}\left(\lambda_n\right)\right]\)</span> 转化成直接对卷积核 <span class="math inline">\(\operatorname{diag}\left[\theta_1, \ldots, \theta_n\right]\)</span>学习 ，不需要再将卷积核进行傅里叶变换，直接将变换后的参数进行学习。 <span class="math display">\[\boldsymbol{y}_{output} = \sigma(\boldsymbol{\Phi}  \boldsymbol{g}_{\theta} \boldsymbol{\Phi}^T \boldsymbol{x})=\sigma(\boldsymbol{\Phi}  \text{diag}[\theta_1,…,\theta_n] \boldsymbol{\Phi}^T \boldsymbol{x})\]</span> 其缺陷在于：</p><ul><li>每一次前向传播，都要计算 <span class="math inline">\(\Phi\)</span>，<span class="math inline">\(diag(\theta_n)\)</span> 及 <span class="math inline">\(\Phi^T\)</span> 三者的矩阵乘积，且需要对拉普拉斯矩阵 <span class="math inline">\(L\)</span> 进行谱分解求 <span class="math inline">\(\boldsymbol{\Phi}\)</span>，Graph很大时复杂度较高</li><li>卷积核不具有spatial localization，最终经过多次矩阵相乘，大部分位置元素不为0，是 global 全连接的卷积核。</li><li>卷积核的参数为 <span class="math inline">\(n\)</span></li></ul></li><li><p><strong>第二代 GCN</strong><a href="https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a> ：<span class="math inline">\(k\)</span> 阶多项式</p><p>把 <span class="math inline">\(\hat{h}\left(\lambda_l\right)\)</span> 设计为 <span class="math inline">\(\sum_{k=0}^K \theta_k \lambda_l^k\)</span>。<strong>图傅里叶变换</strong>是关于特征值（频率）的函数<span class="math inline">\(F\left(\lambda_1\right), \ldots, F\left(\lambda_n\right)\)</span>, 即 <span class="math inline">\(F(\boldsymbol{\Lambda})\)</span> ，将上述卷积核 <span class="math inline">\(\boldsymbol{g}_\theta\)</span> 写作 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> ，将 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> 定义成如下 <span class="math inline">\(k\)</span> 阶多项式形式： <span class="math display">\[\boldsymbol{g}_{\boldsymbol{\theta}^{\prime}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k\]</span> 代入可以得到: <span class="math display">\[\begin{aligned}\boldsymbol{g}_{\boldsymbol{\theta}^{\prime}} * \boldsymbol{x} &amp; \approx \boldsymbol{\Phi} \sum_{k=0}^K \theta_k^{\prime} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T \boldsymbol{x} \\&amp;=\sum_{k=0}^K \theta_k^{\prime}\left(\mathbf{\Phi} \boldsymbol{\Lambda}^k \boldsymbol{\Phi}^T\right) \boldsymbol{x} \\&amp;=\sum_{k=0}^K \theta_k^{\prime}\left(\mathbf{\Phi} \boldsymbol{\Lambda} \boldsymbol{\Phi}^T\right)^k \boldsymbol{x} \\&amp;=\sum_{k=0}^K \theta_k^{\prime} \boldsymbol{L}^k \boldsymbol{x}\end{aligned}\]</span></p><p>其中 <span class="math inline">\(\theta_0, \theta_1, \ldots,\theta_k\)</span> 是需要学习的参数，通过初始化赋值然后利用误差反向传播进行调整。</p><ul><li>优点在于<ul><li>卷积核只有 <span class="math inline">\(k\)</span> 个参数，一般 <span class="math inline">\(k\)</span> 远小于 <span class="math inline">\(n\)</span>，参数的复杂度被大大降低了。</li><li>该公式无需做特征分解，直接对拉普拉斯矩阵 <span class="math inline">\(L\)</span> 进行 <span class="math inline">\(k\)</span> 次方计算即可，降低时间复杂度</li><li>卷积核具有很好的 <strong>spatial localization</strong>，矩阵 <span class="math inline">\(k\)</span> 次方的物理含义：图的<strong>「 k-hop 连通性」</strong>，元素对应1个节点经过 <span class="math inline">\(k\)</span> 步能否到达另一个顶点，非 0 的话可达，为 0 不可达。在卷积核中， <span class="math inline">\(k\)</span> 就是对应的 receptive field。每次卷积会将中心顶点 K-hop neighbor 上的 feature 进行加权求和，权系数就是 <span class="math inline">\(\theta_k\)</span></li></ul></li><li><strong>ChebNet：</strong>引入<strong>切比雪夫展开式</strong>，切比雪夫多项式是以递归方式定义的一系列正交多项式序列。利用<span class="math inline">\(T_k(\Lambda)\)</span> 的 <span class="math inline">\(k\)</span> 阶截断获得对 <span class="math inline">\(L^k\)</span> 的近似，进而获得对 <span class="math inline">\(\boldsymbol{g}_{\boldsymbol{\theta}}(\boldsymbol{\Lambda})\)</span> 的近似，来降低时间复杂度。 <span class="math display">\[\boldsymbol{g}_{\boldsymbol{\theta^{\prime}}}(\boldsymbol{\Lambda}) \approx \sum_{k=0}^K \theta_k^{\prime} T_k(\tilde{\boldsymbol{\Lambda}})\]</span> 切比雪夫多项式 <span class="math inline">\(T_k(\widetilde{\Lambda})\)</span> 使用递归的方式进行定义 <span class="math display">\[T_0(\widetilde{\Lambda})=\mathrm{I}, T_1(\widetilde{\Lambda})=\widetilde{\Lambda}, T_k(\widetilde{\Lambda})=2 \widetilde{\Lambda} T_{k-1}(\widetilde{\Lambda})-T_{k-2}(\widetilde{\Lambda})\]</span> where <span class="math inline">\(\tilde{\Lambda}=\frac{2 }{\lambda_{\max }}\Lambda-\mathrm{I}, \quad \tilde{\lambda} \in[-1,1]\)</span></li></ul></li><li><p><strong>第三代GCN</strong> <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a> ：进一步简化 ChebNet</p><p>在 ChebNet 的基础上取 <span class="math inline">\(k=1, \lambda_{max}=2,\theta = \theta_0^{\prime}=-\theta_1^{\prime}\)</span>，此时模型是1阶的 first-order proximity。即每层卷积层只考虑了直接邻域，类似CNN中 3*3 的卷积核 <span class="math display">\[\begin{aligned} \boldsymbol{g}_{\boldsymbol{\theta^{\prime}}} * \boldsymbol{x} &amp;\approx \theta_0^{\prime} \boldsymbol{x} + \theta_1^{\prime}(\boldsymbol{L}- \boldsymbol{I}_n) \boldsymbol{x} \\ &amp;= \theta_0^{\prime} \boldsymbol{x} - \theta_1^{\prime}(\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}) \boldsymbol{x}\\&amp;= \theta(\boldsymbol{I_n} + \boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}) \boldsymbol{x}\end{aligned}\]</span> 推导中使用了 <strong>归一化的拉普拉斯矩阵</strong> <span class="math display">\[\boldsymbol{L}=\boldsymbol{D}^{-1/2}(\boldsymbol{D}-\boldsymbol{A})\boldsymbol{D}^{-1/2}=\boldsymbol{I_n}-\boldsymbol{D}^{-1/2} \boldsymbol{A} \boldsymbol{D}^{-1/2}\]</span> 此时只有两个参数，即每个卷积核只有2个参数，<span class="math inline">\(\boldsymbol{A}\)</span> 是邻接矩阵。</p><blockquote><p>为什么要归一化？:</p><p>采用加法规则时，对于度大的节点特征越来越大，而对于度小的节点却相反，这可能导致网络训练过程中梯度爆炸或者消失的问题。使用归一化方式，将不再单单地对领域节特征点取平均，它不仅考虑了节点 <span class="math inline">\(i\)</span> 的度，也考虑了邻接节点 <span class="math inline">\(j\)</span> 的度，当邻接节点 <span class="math inline">\(j\)</span> 度数较大时，在聚合时贡献地会更少。</p></blockquote></li></ul><h2 id="benchmark-tasks">6 Benchmark tasks</h2><p>GNN 常应用的任务有以下几类：</p><ul><li><p>classification</p><ul><li>Graph type classification</li><li><p>Edge classification: Traveling Salesman Problem</p></li><li>Semi-supervised node classification<ul><li>pattern recognition</li><li>Semi-supervised graph clustering</li></ul></li></ul></li><li>Regression</li><li>Graph representation learning</li><li><p>Link prediction</p></li></ul><h2 id="references">7 References</h2><ol type="1"><li>蘑菇先生学习记 | 图卷积神经网络理论基础 <a href="http://xtf615.com/2019/02/24/gcn/" class="uri">http://xtf615.com/2019/02/24/gcn/</a></li><li>SivilTaram | 漫谈图神经网络模型 <a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html" class="uri">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></li><li>如何理解 Graph Convolutional Network（GCN）？ - superbrother的回答 - 知乎 https://www.zhihu.com/question/54504471/answer/332657604</li><li>如何理解傅里叶变换公式？ - 苗华栋的回答 - 知乎 https://www.zhihu.com/question/19714540/answer/1119070975</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #01 CNN &amp; RNN</title>
    <link href="/ML-note1-cnn-rnn.html"/>
    <url>/ML-note1-cnn-rnn.html</url>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络-convolution-neural-network">1 卷积神经网络 | Convolution Neural Network</h2><h3 id="why-cnn">1.1 why cnn?</h3><p>一张图片是三维的矩阵，对应的维度为 <span class="math inline">\([width, col, channel]\)</span>。将三维的矩阵 <code>flatten</code> 为 channel 个 <span class="math inline">\(width \times col\)</span> 的长向量，并输入神经网络。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221105225305450.png" alt="image-20221105225305450" style="zoom:40%;"></p><p>这样的做法存在着以下缺点：</p><ul><li><p>全连接参数多</p><p>在全连接前馈神经网络中, 如果第 <span class="math inline">\(l\)</span> 层有 <span class="math inline">\(M_{l}\)</span> 个神经元，第 <span class="math inline">\(l-1\)</span> 层有 <span class="math inline">\(M_{l-1}\)</span> 个 神经元，连接边有 <span class="math inline">\(M_{l} \times M_{l-1}\)</span> 个，权重矩阵有 <span class="math inline">\(M_{l} \times M_{l-1}\)</span> 个参数。当 <span class="math inline">\(M_{l}\)</span> 和 <span class="math inline">\(M_{l-1}\)</span> 都很大时，权重参数非常多，训练效率降低，易出现过拟合。</p></li><li><p>局部不变性</p><p>自然图像中的物体都具有<strong>局部性特征</strong>（如图中猫的耳朵、眼睛，对应物体分类而言是重要的特征）， 而全连接前馈网络很难提取。</p></li></ul><h3 id="结构-structure">1.2 结构 | Structure</h3><p>CNN 并非是对旋转、放大不变的，因此它需要数据增强。</p><h4 id="感受野-receptive-field">感受野 | Receptive field</h4><ul><li><p>stride：感受野步长大小 <span class="math inline">\(S\)</span></p></li><li><p>padding：填充大小 <span class="math inline">\(P\)</span></p></li><li><p>kernel size：感受野大小 <span class="math inline">\(W \times W\)</span></p></li></ul><h4 id="卷积层-convolution">卷积层 | Convolution</h4><ul><li>从神经网络的角度理解：<ul><li>不同的感受野共享参数</li></ul></li><li>从 Filter 的角度理解：<ul><li>使用 Filter 扫整张图片，提取一个局部区域的特征， 不同的卷积核相当于不同的特征提取器</li></ul></li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221106230228014.png" alt="image-20221106230228014" style="zoom:50%;"></p><ul><li>卷积层输出大小：<span class="math inline">\(N = (W − F + 2P)/S + 1\)</span></li></ul><h4 id="池化层-pooling">池化层 | Pooling</h4><ul><li><p>特点：无需学习的参数</p></li><li><p>作用</p><p>进行特征选择，降低特征数量，从而减少参数数量。</p></li><li><p>原因</p><p>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的<strong>神经元个数</strong>并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合、计算复杂度高。 在卷积层之后加上一个汇聚层，从而降低特征维数，避免过拟合。</p></li><li><p>池化函数</p><ul><li>Max Pooling：最大池化，选出最显著的像素</li><li>Mean Pooling：平均池化</li></ul></li></ul><h3 id="图片转换-image-transformation">1.3 图片转换 | Image Transformation</h3><p>平移、旋转、缩放都是在原始对图像矩阵上进行运算。</p><ul><li><p>缩放 <span class="math display">\[\left[\begin{array}{l}x^{\prime} \\y^{\prime}\end{array}\right]=\left[\begin{array}{cc}a &amp; 0 \\0 &amp; d\end{array}\right]\left[\begin{array}{l}x \\y\end{array}\right]+\left[\begin{array}{l}0 \\0\end{array}\right]\]</span></p></li><li><p>旋转，旋转 <span class="math inline">\(\theta\)</span> ° <span class="math display">\[\left[\begin{array}{l}x^{\prime} \\y^{\prime}\end{array}\right]=\left[\begin{array}{cc}\cos \theta &amp; -\sin \theta \\\sin \theta &amp; \cos \theta\end{array}\right]\left[\begin{array}{l}x \\y\end{array}\right]+\left[\begin{array}{l}0 \\0\end{array}\right]\]</span></p></li></ul><h2 id="循环神经网络-rnn">2 循环神经网络 | RNN</h2><p>在许多现实任务中，网络的输出不仅和当前时刻的输入相关， 也和其过去一段时间的输出相关。例如一个有限状态自动机， 其下一个时刻的状态（ 输出） 不仅仅和当前输入相关， 也和当前状态（ 上一个时刻的输出） 相关。</p><p>为了处理时序数据并利用其历史信息， 需要让网络具有短期记忆能力。而前馈网络是一种静态网络， 不具备这种记忆能力，因此提出了循环神经网络 RNN。</p><h3 id="简单循环网络-simple-recurrent-network">2.1 简单循环网络 | Simple Recurrent Network</h3><ul><li><p>有特定的 memory 存储上一个时刻的状态值</p><p>令向量 <span class="math inline">\(\boldsymbol{x}_t \in \mathbb{R}^M\)</span> 表示在时刻 <span class="math inline">\(t\)</span> 时网络的输入， <span class="math inline">\(\boldsymbol{h}_t \in \mathbb{R}^D\)</span> 表示隐藏层状态值，则 <span class="math inline">\(\boldsymbol{h}_t\)</span> 不仅和当前时刻的输入 <span class="math inline">\(\boldsymbol{x}_t\)</span> 相关, 也和上一个时刻的 隐藏层状态 <span class="math inline">\(\boldsymbol{h}_{t-1}\)</span> 相关，更新公式为 <span class="math display">\[\boldsymbol{z}_t=\boldsymbol{U} \boldsymbol{h}_{t-1}+\boldsymbol{W} \boldsymbol{x}_t+\boldsymbol{b}\]</span></p><p><span class="math display">\[\boldsymbol{h}_t=f\left(\boldsymbol{z}_t\right)\]</span></p></li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221108213915842.png" alt="image-20221108213915842" style="zoom: 33%;"></p><ul><li><p>缺点：RNN 每个时刻计算结果后乘 <span class="math inline">\(w\)</span> 作为下一刻的输入，导致最后一次计算的结果形式上是 <span class="math inline">\(w\)</span>​ 累乘。并使用非线性激活函数为 Logistic 函数或 Tanh 函数作为非线性激活函数， 其导数值都小于 1。在建模<strong>长时间间隔（ Long Range）</strong> 的状态之间的依赖关系时，会出现 <strong>梯度消失（gradient vanishing）</strong> 和 gradient explode 的问题。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221109162512582.png" alt="image-20221109162512582" style="zoom: 33%;"></p></li></ul><h3 id="长短期记忆网络-lstm">2.2 长短期记忆网络 | LSTM</h3><p>为了改善循环神经网络的长程依赖问题， 引入门控机制来控制信息的累积速度， 包括有选择地加入新的信息， 并有选择地遗忘之前累积的信息。</p><p>因此，长短期记忆网络（ Long Short-Term Memory Network， LSTM）被提出，主要改进点有两个方面：</p><ul><li>新的内部状态： LSTM网络引入新的内部状态 <span class="math inline">\(C_t\)</span> 专门进行线性的循环信息传递， 同时（ 非线性地） 输出信息给隐藏层的外部状态 <span class="math inline">\(h_t\)</span> ， <span class="math inline">\(C_t\)</span> 计算公式如下：<br><span class="math display">\[\begin{aligned}\boldsymbol{C}_t &amp;=\boldsymbol{f}_t \odot \boldsymbol{C}_{t-1}+\boldsymbol{i}_t \odot \tilde{\boldsymbol{C}}_t \\\boldsymbol{h}_t &amp;=\boldsymbol{o}_t \odot \tanh \left(\boldsymbol{C}_t\right)\end{aligned}\]</span></li></ul><p>其中 $$ 表示向量元素乘积，<span class="math inline">\(\tilde{\boldsymbol{C}}_t\)</span> 是通过非线性函数得到<strong>新的候选状态</strong>，即 new memory <span class="math display">\[\tilde{\boldsymbol{C}}_t = tanh(\boldsymbol{W}_{c}\boldsymbol{x}_t + \boldsymbol{U}_{c}\boldsymbol{h}_{t-1} + \boldsymbol{b}_{c})\]</span> <img src="img/blog/ML-note1-cnn-rnn/image-20221109195443966.png" alt="image-20221109195443966"></p><ul><li><p>门控机制： 门（ gate） 为一个二值变量{0, 1}， 0代表关闭状态， 不许任何信息通过； 1代表开放状态， 允许所有信息通过。LSTM 引入三个“门” 分别为输入门 <span class="math inline">\(\boldsymbol{i}_{t}\)</span>​、遗忘门 <span class="math inline">\(\boldsymbol{f}_{t}\)</span>​ 和输出门 <span class="math inline">\(\boldsymbol{o}_{t}\)</span>。这三个门 是一种<strong>“软” 门</strong>， 取值在 (0, 1) 之间，以<strong>一定的比例</strong>允许信息通过 。</p><ul><li><strong>遗忘门</strong> <span class="math inline">\(\boldsymbol{f}_{t}\)</span> 控制上一个时刻的<strong>内部状态</strong> <span class="math inline">\(\boldsymbol{C}_{t-1}\)</span> 需要<strong>遗忘</strong>多少信息</li><li><strong>输入门</strong> <span class="math inline">\(\boldsymbol{i}_{t}\)</span> 控制当前时刻的<strong>候选状态</strong> $_t $ 有多少信息需要<strong>保存</strong></li><li><strong>输出门</strong> <span class="math inline">\(\boldsymbol{o}_{t}\)</span> 控制当前时刻的<strong>内部状态</strong> <span class="math inline">\(\boldsymbol{C}_{t}\)</span> 有多少信息需要输出给<strong>外部状态</strong> $_t $</li></ul><p><span class="math display">\[\begin{aligned}\boldsymbol{i}_t &amp;=\sigma\left(\boldsymbol{W}_i \boldsymbol{x}_t+\boldsymbol{U}_i \boldsymbol{h}_{t-1}+\boldsymbol{b}_i\right), \\\boldsymbol{f}_t &amp;=\sigma\left(\boldsymbol{W}_f \boldsymbol{x}_t+\boldsymbol{U}_f \boldsymbol{h}_{t-1}+\boldsymbol{b}_f\right) \\\boldsymbol{o}_t &amp;=\sigma\left(\boldsymbol{W}_o \boldsymbol{x}_t+\boldsymbol{U}_o \boldsymbol{h}_{t-1}+\boldsymbol{b}_o\right)\end{aligned}\]</span></p></li></ul><blockquote><p>注意：<span class="math inline">\(\boldsymbol{W}_*\)</span>、<span class="math inline">\(\boldsymbol{U}_*\)</span>、<span class="math inline">\(\boldsymbol{b}_*\)</span>均为可学习的网络参数，也可以将 <span class="math inline">\(\boldsymbol{x}_t\)</span>、<span class="math inline">\(\boldsymbol{h}_{t-1}\)</span> 拼接在一起，共享参数 <span class="math inline">\(\boldsymbol{W}_*\)</span></p></blockquote><p><img src="img/blog/ML-note1-cnn-rnn/image-20221108215916637.png" alt="image-20221108215916637" style="zoom: 33%;"></p><ul><li><p>LSTM是涉及memory 和 input 状态信息的累加，一定程度上解决了 gradient vanishing 问题。</p></li><li><p>与简单循环网络的区别</p><ul><li>简单循环网络中的隐状态 <span class="math inline">\(\boldsymbol{h}\)</span> 存储了历史信息， 每个时刻都会被重写， 是一种短期记忆。</li><li>LSTM 中, <span class="math inline">\(\boldsymbol{h}\)</span> 看作网络参数， 隐含了从训练数据中学到的经验， 其更新周期要远远慢于短期记忆。记忆单元 <span class="math inline">\(\boldsymbol{C}\)</span> 可以在某个时刻捕捉到某个关键信息，并将此关键信息保存一定的时间间隔生命。</li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221109165204933.png" alt="image-20221109165204933" style="zoom:67%;"></p></li></ul><h3 id="gru-门控循环单元">2.3 GRU | 门控循环单元</h3><p>GRU（Gate Recurrent Unit）是 RNN 的一种。和 LSTM 一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的，相比之下<strong>参数量少</strong>，更容易进行训练，提高训练效率。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221123193947597.png" alt="image-20221123193947597" style="zoom: 80%;"></p><ul><li><strong>模型输入：</strong>上一个时刻传输的状态 <span class="math inline">\(h_{t−1}\)</span> 和当前节点的输入 <span class="math inline">\(x_t\)</span> ，相比于 LSTM 只有2个输入</li><li><strong>门控状态：</strong>其中 <span class="math inline">\(r\)</span> 控制重置的门控（reset gate）， <span class="math inline">\(z\)</span> 为控制更新的门控（update gate），update gate的作用类似于input gate和forget gate</li></ul><p><span class="math display">\[\begin{aligned}&amp;z_t=\sigma\left(W_z \cdot\left[h_{t-1}, x_t\right]\right) \\&amp;r_t=\sigma\left(W_r \cdot\left[h_{t-1}, x_t\right]\right) \\\end{aligned}\]</span></p><ul><li><p><strong>新的候选状态：</strong> <span class="math display">\[\hat{h}_t=\tanh \left(W \cdot\left[r_t* h_{t-1}, x_t\right]\right) \\\]</span></p></li><li><p><strong>更新状态：</strong>同时进行了遗忘、记忆两个步骤 <span class="math display">\[h_t=\left(1-z_t\right) * h_{t-1}+z_t * \hat{h}_t\]</span></p><ul><li>特点在于：<strong>使用了同一个门控 z 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）</strong>。</li><li>遗忘 <span class="math inline">\(z\)</span> 和选择 <span class="math inline">\(1−z\)</span> 是联动的。对于传递进来的维度信息，会进行选择性遗忘，遗忘了多少权重 <span class="math inline">\(z\)</span>，就使用包含当前输入的 <span class="math inline">\(\hat{h}_t\)</span> 中所对应的权重进行弥补 <span class="math inline">\(1−z\)</span>。以保持一种”恒定“状态。</li></ul></li></ul><h3 id="应用">2.4 应用</h3><ul><li>序列到类别模式</li><li>同步的序列到序列模式</li><li>异步的序列到序列模式</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ML学习笔记 #00 PyTorch</title>
    <link href="/ML-note0-pytorch.html"/>
    <url>/ML-note0-pytorch.html</url>
    
    <content type="html"><![CDATA[<p>本系列为 Machine Learning 学习笔记，主要记录跟随李宏毅老师 <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">ML 2022 Spring</a> 的学习收获。本文是通过官方快速入门 <a href="https://pytorch.org/tutorials/index.html">PyTorch Tutorials</a>、官方 API 文档 <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a>、<a href="https://www.bilibili.com/video/BV1hE411t7RN/">b站学习视频</a>、<a href="https://youtu.be/85uJ9hSaXig">李宏毅老师机器学习课程</a> 的 PyTorch 学习总结。</p><h2 id="tensor"><a href="https://pytorch.org/docs/stable/tensors.html">Tensor</a></h2><p>类似于 <code>ndarray</code> 的数据结构，构建多维矩阵，可以在 GPU 上训练，并且支持自动微分。</p><ul><li><p>初始化</p><ul><li><p>直接数字构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<span class="hljs-comment"># directly from data</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>从 NumPy array 转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.array([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]))<br></code></pre></td></tr></tbody></table></figure></li><li><p>从另一个 tensor 中构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># datatype 为float</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>特殊构造0、1、随机 tensor</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>主要属性（shape、datatype、device）</p></li><li><p>操作方法：</p><ul><li><p>转移到 GPU 训练：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>tensor = tensor.to(<span class="hljs-string">'cuda'</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>切片操作：类似 <code>numpy</code></p></li><li><p>矩阵堆叠与拼接</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<span class="hljs-comment"># dim = 1，列上相接</span><br>t1 = torch.stack([tensor, tensor, tensor], dim=<span class="hljs-number">0</span>)<span class="hljs-comment"># dim = 0，行上相接</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>乘法运算</p><ul><li><p><code>element-wise product</code> 对应位置的数字相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = tensor.mul(tensor) <br>t2 = tensor * tensor<br></code></pre></td></tr></tbody></table></figure></li><li><p>矩阵相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t3 = tensor.matmul(tensor.T)<br>t3 = tensor @ tensor.T<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>压缩与扩张</p><ul><li><p><code>torch.squeeze()</code>：删除矩阵中大小为1的所有维度</p><p>例如输入 <span class="math inline">\(A \times 1 \times B \times C \times 1 \times D\)</span>，转换为 <span class="math inline">\(A \times B \times C \times D\)</span></p></li><li><p><code>torch.unsqueeze()</code>：在指定位置插入维度为的张量</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>torch.unsqueeze(x, <span class="hljs-number">0</span>)<br>torch.unsqueeze(x, <span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>沿着某维度复制张量 <code>repeat</code></p><ul><li><p>参数是对应维度的复制个数，上段代码为0维复制两次，1维复制两次，则得到以上运行结果。其余扩展情况依此类推</p></li><li><p>repeat参数个数与 tensor 维数一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(b.shape)<span class="hljs-comment"># 得到结果torch.Size([4, 6])</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>repeat参数个数与tensor维数不一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([1, 4, 3])</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([2, 2, 3])</span><br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p><strong>就地操作 In-place operations：</strong></p><ul><li>操作影响 被操作 tensor 本身的值发生变化，即修改内存</li><li>可以节省内存，但在计算导数可能会出现问题，不推荐使用</li></ul><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>.add_(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>自动求导：<code>torch.autograd</code></p><ul><li><p>支持自动微分：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>], [-<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>backward</code>：计算给定张量相对于自变量的梯度总和</p></li><li><p><code>grad</code>：相对于输入的输出梯度的总和。</p></li></ul><p><img src="img/blog/ML-note0-pytorch/image-20221101204244079.png" alt="image-20221101204244079" style="zoom:50%;"></p></li></ul><h2 id="数据加载-datasets-dataloaders">数据加载 | <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></h2><h3 id="dataset">dataset</h3><ul><li><p>获取和加载数据集</p><ul><li>PyTorch 中提供了许多预加载的数据集（如 FashionMNIST），通过调用函数即可加载</li><li><code>torchvision</code> 库：图片数据集 <a href="https://pytorch.org/vision/stable/datasets.html">Image Datasets</a></li><li><code>torchtext</code> 库：文本数据集 <a href="https://pytorch.org/text/stable/datasets.html">Text Datasets</a></li><li><code>torchaudio</code> 库：音频信号数据集 <a href="https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,<span class="hljs-comment"># root是存储训练/测试数据的路径，</span><br>    train=<span class="hljs-literal">True</span>,<span class="hljs-comment"># train指定训练或测试数据集，</span><br>    download=<span class="hljs-literal">True</span>,<span class="hljs-comment"># 下载数据</span><br>    transform=ToTensor()<span class="hljs-comment"># feature and label transformations</span><br>)<br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>创建自定义数据集，继承 <code>Dataset</code> 类，重写 <code>init</code>、<code>getitem</code>、<code>len</code> 方法</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, file</span>):<br>self.data = ...<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br><span class="hljs-keyword">return</span> self.data[index]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br></code></pre></td></tr></tbody></table></figure></li></ul><h3 id="dataloader">dataloader</h3><ul><li><p>进行数据加载的功能，数据集分为“小批量”，是否需要乱序打乱</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>迭代每个元素</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br></code></pre></td></tr></tbody></table></figure></li></ul><figure><img src="./img/blog/ML-note0-pytorch/image-20221031223432180.png" alt="image-20221031223432180"><figcaption>image-20221031223432180</figcaption></figure><h2 id="tensorboard-可视化"><a href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a> | 可视化</h2><ul><li><p>可视化数据</p><ul><li><code>add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=‘CHW’)</code>：绘制图片，可用于检查模型的输入，监测 feature map 的变化，或是观察 weight。</li></ul></li><li>可视化模型内部的 layer<ul><li><code>add_graph(model, input_to_model=None, verbose=False, use_strict_trace=True)</code>：每个 layer 的输入、输出维度</li></ul></li><li>可视化神经网络模型训练过程、结果。<ul><li><code>add_scalars(tag, scalar_value, global_step=None)</code></li></ul></li></ul><h2 id="transforms"><a href="https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html">Transforms</a></h2><ul><li><p>作用：对图像进行归一化 <code>Normalize</code>、旋转 <code>rotate</code>、裁剪 <code>resize</code>、灰度等</p></li><li><p>输入：<code>PIL</code> 库的方法 <code>Image.open()</code>；<code>opencv</code> 的方法 <code>cv.imread()</code> 输入图像</p></li><li><p><code>ToTensor()</code></p><p>将 PIL 图像或 NumPyinto 转换为 <code>tensor</code>，并缩放图像的像素强度值在 [0.， 1.] 范围内。</p></li><li><p>将所有变换组合在一起</p></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">transforms.Compose([<br>     transforms.CenterCrop(<span class="hljs-number">10</span>),<br>     transforms.PILToTensor(),<br>     transforms.ConvertImageDtype(torch.<span class="hljs-built_in">float</span>),<br>])<br></code></pre></td></tr></tbody></table></figure><h2 id="神经网络">神经网络</h2><h3 id="nn"><a href="https://pytorch.org/docs/stable/nn.html">NN</a></h3><ul><li><p><code>module</code>：构建所有神经网络的基类，需要进行继承</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<span class="hljs-comment"># 模型和层的初始化</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.conv1(x))<br>        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))<span class="hljs-comment"># 计算输出</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>已实现的模型直接调用：</p><ul><li><p><a href="https://pytorch.org/vision/stable/models.html">Models and pre-trained weights — Torchvision</a>：包括 <a href="https://pytorch.org/vision/stable/models/alexnet.html">AlexNet</a>、<a href="https://pytorch.org/vision/stable/models/vgg.html">VGG</a> 等</p><ul><li><p>可以选择是否使用预训练好的参数、原有的转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize model with the best available weights</span><br>weights = ResNet50_Weights.DEFAULT<br>model = resnet50(weights=weights)<br><span class="hljs-comment"># No weights - random initialization</span><br>model = resnet50(weights=<span class="hljs-literal">None</span>)<br><br><span class="hljs-comment"># model weight includes preprocessing transforms</span><br>preprocess = weights.transforms()<br><span class="hljs-comment"># Apply it to the input image</span><br>img_transformed = preprocess(img)<br></code></pre></td></tr></tbody></table></figure></li><li><p>可以对已经定义好的模型进行添加、修改 layer：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>vgg16.classifier.add_module(<span class="hljs-string">'add_linear'</span>, nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># 修改分类为10类</span><br>vgg16.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li></ul></li><li><p><code>Sequential</code>：将不同神经网络层进行顺序拼接，将整个容器视为单个模块，避免手动多次调用</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">self.net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>),<br>    nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>loss function</code>：</p><ul><li>Mean Squared Error（回归任务）：<code>criterion = nn.MSELoss()</code></li><li>Cross Entropy （分类任务）：<code>criterion = nn.CrossEntropyLoss()</code><ul><li>input 为 <span class="math inline">\((batch Size, class)\)</span>，对应每个 class 的输出概率</li><li>target 可以为 <span class="math inline">\((batch Size, 1)\)</span>，对应每个 sample 的 <code>class index</code>；也可以为 <span class="math inline">\((batch Size, class)\)</span>，对应类概率</li></ul></li><li>计算损失：<code>loss = criterion(model_output, expected_value)</code></li></ul></li></ul><h3 id="优化-optim">优化 | <a href="https://pytorch.org/docs/stable/optim.html">OPTIM</a></h3><p>实现各种优化算法的软件包。</p><ul><li><p>构造优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br>optimizer = optim.Adam([var1, var2], lr=<span class="hljs-number">0.0001</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>采取优化步骤：<code>optimizer.step()</code></p></li><li><p>结合训练过程的优化</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()<span class="hljs-comment"># 把上一个循环的梯度清0</span><br>    output = model(<span class="hljs-built_in">input</span>)<br>    loss = loss_fn(output, target)<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></tbody></table></figure></li><li><p>正则化：设置 <code>weight_decay &gt; 0</code> ，pytorch 自动完成正则化计算</p></li></ul><h3 id="神经网络训练-step">神经网络训练 | Step</h3><ul><li><p>定义：数据集、数据批次、模型、损失函数、优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = MyDataset(file)<span class="hljs-comment"># read data via MyDataset</span><br>tr_set = DataLoader(dataset, <span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<span class="hljs-comment"># put dataset into Dataloader</span><br>model = MyModel().to(device)<span class="hljs-comment"># construct model and move to device </span><br>criterion = nn.MSELoss()<span class="hljs-comment"># set loss function</span><br>optimizer = torch.optim.SGD(model.parameters(), <span class="hljs-number">0.1</span>)<span class="hljs-comment"># set optimizer</span><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>) <span class="hljs-comment"># set gpu training</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>训练过程：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tensorboard</span><br>writer = SummaryWriter(<span class="hljs-string">"../logs"</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<span class="hljs-comment"># iterate n_epochs</span><br>model.train()<span class="hljs-comment"># set model to train mode</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> tr_set: <span class="hljs-comment"># iterate through the dataloader</span><br>optimizer.zero_grad()<span class="hljs-comment"># set gradient to zero</span><br>        x, y = x.to(device), y.to(device)<span class="hljs-comment"># move data to device (cpu/cuda)</span><br>        pred = model(x)<span class="hljs-comment"># forward pass (compute output)</span><br>        loss = criterion(pred, y)<span class="hljs-comment"># compute loss</span><br>        loss.backward()<span class="hljs-comment"># compute gradient (backpropagation)</span><br>        optimizer.step()<span class="hljs-comment"># update model with optimizer</span><br>    writer.add_scalar(<span class="hljs-string">"train_loss"</span>,loss.item(),epoch)<br></code></pre></td></tr></tbody></table></figure></li><li><p>验证过程：无需进行优化，只计算 <code>loss</code> 即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<span class="hljs-comment"># 开启验证模式</span><br>total_loss = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> dv_set:<br>    x, y = x.to(device), y.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<span class="hljs-comment"># 关闭梯度计算</span><br>        pred = model(x)<br>        loss = criterion(pred, y)<br>        total_loss += loss.cpu().item() * <span class="hljs-built_in">len</span>(x)<br>        avg_loss = total_loss / <span class="hljs-built_in">len</span>(dv_set.dataset)<br></code></pre></td></tr></tbody></table></figure></li><li><p>测试阶段：无需优化、计算损失，只需预测正确答案即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<br>preds = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tt_set:<br>    x = x.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        pred = model(x)<br>        preds.append(pred.cpu())<br></code></pre></td></tr></tbody></table></figure></li><li><p>存储模型</p><ul><li><p>Save：常用的方法是以字典的形式保存模型参数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">path = <span class="hljs-string">'model.pth'</span><br><span class="hljs-comment"># 存储模型训练好的参数、模型结构</span><br>torch.save(model, path)<br><span class="hljs-comment"># 存储模型训练好的参数</span><br>torch.save(model.state_dict(), path)<br></code></pre></td></tr></tbody></table></figure></li><li><p>Load：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模型训练好的参数、模型结构</span><br>model = torch.load(path)<br><span class="hljs-comment"># 加载模型训练好的参数</span><br>model = NeuralNetwork()<br>model.load_state_dict(ckpt)  <br></code></pre></td></tr></tbody></table></figure><blockquote><p>注意第一种方法：网络模型定义的代码要与 load 代码写在一起，或者 <code>from model import *</code>，这样才能成功加载</p></blockquote></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>保研小记</title>
    <link href="/Baoyan-essay.html"/>
    <url>/Baoyan-essay.html</url>
    
    <content type="html"><![CDATA[<h1 id="保研小记">保研小记</h1><div class="note note-info">            <p>一切都是最好的安排。</p>          </div><h2 id="写在前面">0. 写在前面</h2><p>大三，身处于人生的岔路口，无论是出国、工作、保研、考研的每一位同学，在答案没有正式揭晓前，都面临着选择与被选择。谨以此博客记录我曾经焦虑、一度感到非常煎熬的保研季，算是正式的说再见 <span class="github-emoji" style="display:inline;vertical-align:middle"><span>👋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>。</p><h2 id="个人情况">1. 个人情况</h2><ul><li><p>本科：普通中九计算机专业</p></li><li>排名：2%</li><li>获奖情况：一次国奖</li><li>竞赛经历：数学建模国一（自我认为竞赛方面除了 ACM 以外的奖项都不太能发挥太大用处。）</li><li>科研经历：无产出（这点真的在组面的时候很吃亏，一下就会被学术能力强的老师淘汰。）</li><li><p>最终去向：复旦大学，学硕</p></li></ul><h2 id="最初定位">2. 最初定位</h2><ul><li><p>院校选择：定位是华五，没敢奢求清北。心仪的院校是复旦、人大高瓴、自动化所。</p></li><li><p>老师选择：在组面的过程中确实也被磨平了心志，科研经历比不上有论文发表的大佬，觉得卷不过别人，只希望找到对应目标方向、人品较好的导师。</p></li><li>方向：AI 方向。</li><li><p>填报学位：没做好直博的准备，倾向学硕，专硕也可以接受（顾虑主要是专硕有些学校学费高且无宿舍）。</p></li></ul><h2 id="时间线">3. 时间线</h2><div class="note note-success">            <p>越早越好:)</p>          </div><ol type="1"><li>简历准备：大概是2月底3月初，后续也一直在更新简历的内容。<strong>简历上的每个点都要完全掌握。</strong></li><li>发邮件给老师：5月初才开始发了第一封（算是很晚的，有些学校完全是老师决定入营的）。<strong>3-4月联系意向导师完成project / 提前面试 → 院校夏令营报名前 → 入营后面试前 → 结果出来后</strong>，我觉得是都是联系老师的重要时间点，不用对老师没回邮件特别失落（老师收到的邮件确实很多，可以尝试再发一次）。</li><li>材料准备：5月中~6月初，差不多就是在夏令营填报过程中慢慢完善的</li><li>复习：6月，因为是AI方向，因此我大概复习的是数学基础课、数据结构和机器学习，准备英语问题。</li><li>夏令营正式开始： 7.1-7.24，这段是最辛苦的时候</li><li>保研间隙：8月，因为有了保底 offer 所以很纯粹地休息了一个月。后面深入了解了一下 offer 院校和导师的具体情况，还是觉得休息躺平早了…</li><li>预推免：9.13-9.15</li></ol><h2 id="夏令营情况">4. 夏令营情况</h2><p>入营大部分都是<strong>靠 rank 筛选</strong>，是否优营基本取决于面试，比较看重<strong>科研经历、机试能力</strong>，这些都需要提前的准备，丰富自己的简历。如果在本科期间，能与本校 / 外校导师实验室做科研并有所产出，在面试中取胜概率会大很多。</p><table><thead><tr class="header"><th>学校</th><th>学院/实验室名称</th><th>是否入营</th><th>结果</th></tr></thead><tbody><tr class="odd"><td>复旦</td><td>计算机学院，人工智能学硕</td><td>是</td><td>在 waiting list 中等位置，最终候补到（最终去向）</td></tr><tr class="even"><td>北大</td><td>软微专硕</td><td>是</td><td>优营</td></tr><tr class="odd"><td>北大</td><td>计算机学院，学硕</td><td>否</td><td>-</td></tr><tr class="even"><td>人大</td><td>高瓴，学硕</td><td>是</td><td>在 waiting list 中等偏后了</td></tr><tr class="odd"><td>国科大</td><td>自动化所，学硕</td><td>是</td><td>优营，无双选老师自动放弃</td></tr><tr class="even"><td>南大</td><td>人工智能学院，学硕</td><td>是</td><td>参加笔试通过了，面试时间冲突放弃。</td></tr><tr class="odd"><td>中科大</td><td>网安学院，学硕</td><td>是</td><td>优营，放弃</td></tr><tr class="even"><td>上交</td><td>电院，计算机方向学硕</td><td>是</td><td>时间冲突放弃</td></tr><tr class="odd"><td>国科大</td><td>软件所，学硕</td><td>否</td><td>-</td></tr></tbody></table><h3 id="复旦cs">复旦CS</h3><p>入营要求比较高，卡 rank 的，联系老师也应该没办法。300人入营，发放的名额非常少。入营后联络员通知拉进微信群，送了一件T恤和复旦大学的本子。</p><p>第一天是听各个方向的宣讲，感觉还挺重要的，有些老师会对自己实验室情况详细介绍，可以听下感兴趣的。晚上进行<strong>志愿的填报</strong>，这个很重要，算是大型<strong>博弈场</strong>，后续优营的评选、候补的情况完全都是在你<strong>填报的志愿队列排序</strong>。系统实时更新，可以看到当前报考人数/招生人数。附上一张最后的填报图，可以看出竞争真的很激烈。</p><p><img src="/img/post_img/志愿.png"></p><p>第二天上午是复旦的机考，2h3题。<strong>不设监考，不计入总分，但是面试会看到成绩，会问解题思路</strong>。OJ 系统评测，题目难度很大。下午是一个英语的面试流程，6-8min。问的范围蛮广的，需要提前准备。</p><p>第三天是专业面试，是最重要的，15min左右。先是两分钟的中文自我介绍。接下来问你的机试某道题的解题思路、时间复杂度、有没有更优解等。之后都问具体方向的问题，<strong>每个方向侧重问的内容不一</strong>，机器学习以及408均有可能。</p><h3 id="人大高瓴">人大高瓴</h3><p>入营要求高，也是卡 rank 的，学硕名额最终只有24个，比去年少了。整个流程可能为了杜绝作弊，保证公平性，步骤繁杂、有大量等候流程、非常浪费时间、教务老师也比较严格，参营体验真的不太好。</p><p>开营老师和学生都会来做介绍发言，高瓴有非常多年轻学术能力强的老师，值得一听。具体的考核签了比较严格的保密协议，无法透露，只能说是本人参与的<strong>难度最大</strong>的笔试和面试。</p><p>笔试：1.5h，<strong>官方发布的范围</strong>是数学（线性代数、概率论等）、程序设计、数据结构与算法（75分）。</p><p>面试：15min，同一组序号靠后的同学可能要等很久，并且等待过程中需要开着视频什么都不能做。<strong>官方发布的范围</strong>是数学、计算机、人工智能基础（75分）、 英语听、说及综合能力（50分）。</p><h3 id="自动化所">自动化所</h3><p>今年入营要求高，也是卡 rank 的。虽然是线上，但是真的能感受到学长学姐真正带动起群里的氛围，比较温馨，确实最像是一个夏令营的营。有连续几天的学术报告会和交流会，中间穿插一个心理测试，题量大，正常答就行。</p><p>只有面试，1人12min。先是自我介绍1min，简单英语问题。而后是数学概念题，<strong>高数、线代、概率论的基础概念</strong>很重要，不难，但是一定要提前复习。接下来都是围绕简历项目的内容展开。</p><p>优营率很高，貌似分为学硕 / 直博 / 专硕三个档次发放，感觉大部分参加的同学都拿到了Offer，但是必须选到老师才有效。后续需要「与老师双选签订协议」，这个我认为是自动化所真正的考核，热门导师的竞争比较激烈，每个老师只有1-2个名额（一般博导名额是博士，硕导名额是硕士）。需要 <strong>发邮件 → 课题组一起面试 → 确定是否双选 → 签协议</strong> 一系列过程，因此收到学院的 offer 远远没有结束考核。本人因为被几个课题组接连拒绝，比较心灰意冷，也不再另找老师签协议了。</p><h3 id="北大软微">北大软微</h3><p>一开始并不在目标院校的学院中，北大的报名材料是最多的，一向非常繁琐，需要另准备推荐信等。</p><p>变化比较大的是，一向以title、放养、就业导向闻名的软微学院，在开营招生会上宣布今年改成<strong>科研导向</strong>，学院说只给3-6个月实习，给的补助大部分能够 cover 3年9w的学费，无法知道后续具体政策落实如何。软微应该是靠 rank 初筛，然后需要选择对应方向的论文做汇报、面试。我填报的方向基本不考专业知识，只问了论文汇报、个人情况。面试完后可能会有导师电话联系，给<strong>非常短的时间</strong>让你确定来不来，一般和老师确定来的都能给 offer，优营率70%以上。这个阶段需要<strong>比较谨慎</strong>，因为软微大多数导师不做科研，老师之间差别比较大，可以直接深入询问老师是否放实习、培养方案、组里方向、科研or横向等。</p><ul><li><strong>优点：</strong>算是清北中门槛最低的一个院校，难度小。因为 pku 的 title，体制内有优势。往年因为放实习，就业率也非常好，是就业向同学非常好的选择。</li><li><strong>缺点：</strong>学费高，地处大兴，可能研二以后需要两头跑，宿舍不太方便住。科研向的同学能找到的老师非常少。</li></ul><h3 id="南大ai">南大AI</h3><p>相比起来，个人感觉教务老师很nice，应该是只允许入一个方向的营。因为时间冲突只参加了笔试。题量比较多，考了很多机器学习的知识，会先通过笔试筛掉一部分人，再确定面试的名单。</p><h3 id="中科大网安">中科大网安</h3><p>老师可能在多个学院有招生名额，除了学位外差别不大。因为联系的导师在网安招生，因此报了网安学院。点名表扬中科大，入营后发放了大礼包，真的诚意满满。中科大一向入营要求不高，优营也是发很多（毕竟科大因为在合肥，鸽子实在太多了），但是科大真的有很多实力强劲的导师，提前联系希望非常大。</p><p>无机试无笔试，为了公平，面试有两轮，流程一模一样，只是老师不一样。需要抽题号问专业知识，因为我本身没有学过 「网安」方向的任何知识，基本答不上来，联系的导师说没关系，可以回答没学过。</p><h2 id="预推免情况">5. 预推免情况</h2><p>因为夏令营有了 offer，一度也觉得比较满意，但是考虑到软微的总总不足，还是准备冲一下。因此预推免只报了清华大学深圳研究生院的考核。</p><h3 id="清深cs">清深CS</h3><p>机试：3题300分，题目是和本部贵系一样的，难度大。</p><p>综合面：共10分钟，读论文并翻译，老师问两个英文问题，然后中文问些心理等家常问题。</p><p>专业面：共10分钟，PPT自我介绍3-5分钟，老师围绕你的简历展开，可能很看你做的东西和老师方向会不会 match，本人和老师聊的完全不在一个频道上。</p><h2 id="总结">6. 总结</h2><h3 id="最终结果">最终结果</h3><p>夏令营留的一个 offer 只有软微。但是因为最开始想去的确实是复旦大学，并结合了往年鸽子较多的情况，在拿了 waiting list 后也询问了具体的排名，残留候补上的希望。清深结果9月19日上午出的，很意外居然拿到了offer，相比软微更倾向清深，因此也接受了预录取资格。没想到9月19日晚上复旦大学招生办打电话通知我候补上了，曾一度以为和上海无缘失去希望，最终也很惊喜，并且联系到学术好人品好的导师，算是圆梦了。在当天晚上就释放了清深的名额，第二天也和软微的老师说明了情况，老师表示理解。我也不喜欢这样的鸽子行为，深感抱歉，只能尽可能降低对老师们的影响了。</p><p>至此，整个持续好几个月的保研应该完整地结束了，无论是院校还是老师，我都觉得非常满意，算是一个圆满的句号。是很艰难的过程，一路上会看到各方面实力都碾压自己的大佬，一度非常质疑自己，被老师拒绝到怀疑人生，时间安排紧复习来不及等问题，最终还是熬过来了。真心感谢一路帮助我的老师、学长学姐，以及一起参加保研的小伙伴~</p><p>关于<strong>「鸽子与海王」</strong>的现象，恶意收集 offer，等到最后再鸽真的很不好。个人认为手上有2个 offer 即可，如果拿到的1个 offer 不满意，可以作为保底再报一个冲一冲，但是真的需要尽早释放，邮件和招生办的老师说明不去、请求老师的谅解等等。</p><h3 id="保研心得">保研心得</h3><div class="note note-success">            <p>华五强导-&gt;要求高-&gt;大佬才能拿到offer-&gt;大佬大概率能去清北-&gt;鸽-&gt;有机会补录上</p>          </div><p>最后的最后，送上一份网上流传很广的某学校招生老师的鸡汤共勉，</p><blockquote><p>在每位同学漫长的人生里，会经历很多这样的时刻，我们有时做自己的分子，有时做别人的分母。但无论如何，都需要以做分母的心态，尽做分子的努力。</p><p>从研究方法的角度来讲，我们抽取了一个很小样本：从诸位21年的生活里面抽取了几天，从你所有的知识库里面抽取了几个问题，从你无数侧面里抽取了一面，这里没法测量你的curiosity，personality等等宝贵的品质。</p><p>我们只能在此公布一个结果，但任何一个结果都不能最终定义你。</p><p>能定义你的，只有你自己。</p></blockquote><p>以及，一切都是最好的安排，顺其自然就好。上岸不是终点，保持一以贯之的努力，每天的微小积累才是决定漫漫人生路的因素~</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello My Blog Space</title>
    <link href="/hello-world.html"/>
    <url>/hello-world.html</url>
    
    <content type="html"><![CDATA[<p>本博客参考 <a href="https://hwcoder.top/Hello-My-World#搭建历程">Hello My World | Hwcoder</a> 的搭建过程以及 <a href="https://hexo.fluid-dev.com/docs/guide/#关于指南">Hexo Fluid 配置指南</a> 下完成。主要基于 Hexo + GitHub 搭建，采用 Fluid 主题。</p><h2 id="搭建历程">搭建历程</h2><h2 id="hexo-命令">hexo 命令</h2><h4 id="新建文章">新建文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n <span class="hljs-string">"My New Post"</span> <span class="hljs-comment"># new</span><br></code></pre></td></tr></tbody></table></figure><h4 id="清除本地缓存">清除本地缓存</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo cl  <span class="hljs-comment">#clean</span><br></code></pre></td></tr></tbody></table></figure><h4 id="生成静态文章">生成静态文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo g <span class="hljs-comment"># generate</span><br></code></pre></td></tr></tbody></table></figure><h4 id="阅览文章">阅览文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s <span class="hljs-comment"># show</span><br></code></pre></td></tr></tbody></table></figure><h4 id="部署推送到github">部署推送到github</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo d <span class="hljs-comment"># deploy</span><br></code></pre></td></tr></tbody></table></figure><h2 id="评论功能">评论功能</h2><p>Valine 是国内的一款极简风格的评论软件，也是 Fluid 支持的评论软件之一。在 <code>comment</code> 中选择 <code>valine</code>，之后找到相应的配置区域进行如下操作：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">comments:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">valine</span><br></code></pre></td></tr></tbody></table></figure><p>进入官网 <a href="https://leancloud.cn/">LeanCloud</a> 完成注册，然后在控制台创建一个项目 <code>Blog.Comments</code> 后，通过 <strong>控制台 &gt; 设置 &gt; 应用凭证</strong> 获取密钥（App ID 和 App Key），在对应位置填入。</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">valine:</span><br>  <span class="hljs-attr">appId:</span> <br>  <span class="hljs-attr">appKey:</span> <br></code></pre></td></tr></tbody></table></figure><h3 id="latex-数学公式">Latex 数学公式</h3><p>主要方法依照参考博客，但是在执行 <code>hexo cl</code> 、<code>hexo g</code> 可能会出现报错 <code>pandoc exited with code 9: pandoc: Unknown extension: smart</code>。</p><p>解决办法为将 <code>node_modules\hexo-renderer-pandoc\index.js</code> 中的</p><figure class="highlight js"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> args = [ <span class="hljs-string">'-f'</span>, <span class="hljs-string">'markdown-smart'</span>+extensions, <span class="hljs-string">'-t'</span>, <span class="hljs-string">'html-smart'</span>, math]<br></code></pre></td></tr></tbody></table></figure><p>改成</p><figure class="highlight js"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> args = [ <span class="hljs-string">'-f'</span>, <span class="hljs-string">'markdown'</span>+extensions, <span class="hljs-string">'-t'</span>, <span class="hljs-string">'html'</span>, math]<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术栈</category>
      
      <category>博客</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
