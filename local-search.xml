<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ML学习笔记 #00 PyTorch</title>
    <link href="/ML-note0-pytorch.html"/>
    <url>/ML-note0-pytorch.html</url>
    
    <content type="html"><![CDATA[<p>本系列为 Machine Learning 学习笔记，主要记录跟随李宏毅老师 <a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">ML 2022 Spring</a> 的学习收获。本文是通过官方快速入门 <a href="https://pytorch.org/tutorials/index.html">PyTorch Tutorials</a>、官方 API 文档 <a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a>、<a href="https://www.bilibili.com/video/BV1hE411t7RN/">b站学习视频</a>、<a href="https://youtu.be/85uJ9hSaXig">李宏毅老师机器学习课程</a> 的 PyTorch 学习总结。</p><h2 id="tensor"><a href="https://pytorch.org/docs/stable/tensors.html">Tensor</a></h2><p>类似于 <code>ndarray</code> 的数据结构，构建多维矩阵，可以在 GPU 上训练，并且支持自动微分。</p><ul><li><p>初始化</p><ul><li><p>直接数字构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<span class="hljs-comment"># directly from data</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>从 NumPy array 转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.array([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]))<br></code></pre></td></tr></tbody></table></figure></li><li><p>从另一个 tensor 中构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># datatype 为float</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>特殊构造0、1、随机 tensor</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>主要属性（shape、datatype、device）</p></li><li><p>操作方法：</p><ul><li><p>转移到 GPU 训练：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>tensor = tensor.to(<span class="hljs-string">'cuda'</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>切片操作：类似 <code>numpy</code></p></li><li><p>矩阵堆叠与拼接</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<span class="hljs-comment"># dim = 1，列上相接</span><br>t1 = torch.stack([tensor, tensor, tensor], dim=<span class="hljs-number">0</span>)<span class="hljs-comment"># dim = 0，行上相接</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>乘法运算</p><ul><li><p><code>element-wise product</code> 对应位置的数字相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = tensor.mul(tensor) <br>t2 = tensor * tensor<br></code></pre></td></tr></tbody></table></figure></li><li><p>矩阵相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t3 = tensor.matmul(tensor.T)<br>t3 = tensor @ tensor.T<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>压缩与扩张</p><ul><li><p><code>torch.squeeze()</code>：删除矩阵中大小为1的所有维度</p><p>例如输入 <span class="math inline">\(A \times 1 \times B \times C \times 1 \times D\)</span>，转换为 <span class="math inline">\(A \times B \times C \times D\)</span></p></li><li><p><code>torch.unsqueeze()</code>：在指定位置插入维度为的张量</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>torch.unsqueeze(x, <span class="hljs-number">0</span>)<br>torch.unsqueeze(x, <span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>沿着某维度复制张量 <code>repeat</code></p><ul><li><p>参数是对应维度的复制个数，上段代码为0维复制两次，1维复制两次，则得到以上运行结果。其余扩展情况依此类推</p></li><li><p>repeat参数个数与 tensor 维数一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(b.shape)<span class="hljs-comment"># 得到结果torch.Size([4, 6])</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>repeat参数个数与tensor维数不一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([1, 4, 3])</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([2, 2, 3])</span><br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p><strong>就地操作 In-place operations：</strong></p><ul><li>操作影响 被操作 tensor 本身的值发生变化，即修改内存</li><li>可以节省内存，但在计算导数可能会出现问题，不推荐使用</li></ul><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>.add_(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>自动求导：<code>torch.autograd</code></p><ul><li><p>支持自动微分：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>], [-<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>backward</code>：计算给定张量相对于自变量的梯度总和</p></li><li><p><code>grad</code>：相对于输入的输出梯度的总和。</p></li></ul><p><img src="img/blog/ML-note0-pytorch/image-20221101204244079.png" alt="image-20221101204244079" style="zoom:50%;"></p></li></ul><h2 id="数据加载-datasets-dataloaders">数据加载 | <a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></h2><h3 id="dataset">dataset</h3><ul><li><p>获取和加载数据集</p><ul><li>PyTorch 中提供了许多预加载的数据集（如 FashionMNIST），通过调用函数即可加载</li><li><code>torchvision</code> 库：图片数据集 <a href="https://pytorch.org/vision/stable/datasets.html">Image Datasets</a></li><li><code>torchtext</code> 库：文本数据集 <a href="https://pytorch.org/text/stable/datasets.html">Text Datasets</a></li><li><code>torchaudio</code> 库：音频信号数据集 <a href="https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,<span class="hljs-comment"># root是存储训练/测试数据的路径，</span><br>    train=<span class="hljs-literal">True</span>,<span class="hljs-comment"># train指定训练或测试数据集，</span><br>    download=<span class="hljs-literal">True</span>,<span class="hljs-comment"># 下载数据</span><br>    transform=ToTensor()<span class="hljs-comment"># feature and label transformations</span><br>)<br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>创建自定义数据集，继承 <code>Dataset</code> 类，重写 <code>init</code>、<code>getitem</code>、<code>len</code> 方法</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, file</span>):<br>self.data = ...<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br><span class="hljs-keyword">return</span> self.data[index]<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br></code></pre></td></tr></tbody></table></figure></li></ul><h3 id="dataloader">dataloader</h3><ul><li><p>进行数据加载的功能，数据集分为“小批量”，是否需要乱序打乱</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>迭代每个元素</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br></code></pre></td></tr></tbody></table></figure></li></ul><figure><img src="./img/blog/ML-note0-pytorch/image-20221031223432180.png" alt="image-20221031223432180"><figcaption>image-20221031223432180</figcaption></figure><h2 id="tensorboard-可视化"><a href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a> | 可视化</h2><ul><li><p>可视化数据</p><ul><li><code>add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=‘CHW’)</code>：绘制图片，可用于检查模型的输入，监测 feature map 的变化，或是观察 weight。</li></ul></li><li>可视化模型内部的 layer<ul><li><code>add_graph(model, input_to_model=None, verbose=False, use_strict_trace=True)</code>：每个 layer 的输入、输出维度</li></ul></li><li>可视化神经网络模型训练过程、结果。<ul><li><code>add_scalars(tag, scalar_value, global_step=None)</code></li></ul></li></ul><h2 id="transforms"><a href="https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html">Transforms</a></h2><ul><li><p>作用：对图像进行归一化 <code>Normalize</code>、旋转 <code>rotate</code>、裁剪 <code>resize</code>、灰度等</p></li><li><p>输入：<code>PIL</code> 库的方法 <code>Image.open()</code>；<code>opencv</code> 的方法 <code>cv.imread()</code> 输入图像</p></li><li><p><code>ToTensor()</code></p><p>将 PIL 图像或 NumPyinto 转换为 <code>tensor</code>，并缩放图像的像素强度值在 [0.， 1.] 范围内。</p></li><li><p>将所有变换组合在一起</p></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">transforms.Compose([<br>     transforms.CenterCrop(<span class="hljs-number">10</span>),<br>     transforms.PILToTensor(),<br>     transforms.ConvertImageDtype(torch.<span class="hljs-built_in">float</span>),<br>])<br></code></pre></td></tr></tbody></table></figure><h2 id="神经网络">神经网络</h2><h3 id="nn"><a href="https://pytorch.org/docs/stable/nn.html">NN</a></h3><ul><li><p><code>module</code>：构建所有神经网络的基类，需要进行继承</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<span class="hljs-comment"># 模型和层的初始化</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.conv1(x))<br>        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))<span class="hljs-comment"># 计算输出</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>已实现的模型直接调用：</p><ul><li><p><a href="https://pytorch.org/vision/stable/models.html">Models and pre-trained weights — Torchvision</a>：包括 <a href="https://pytorch.org/vision/stable/models/alexnet.html">AlexNet</a>、<a href="https://pytorch.org/vision/stable/models/vgg.html">VGG</a> 等</p><ul><li><p>可以选择是否使用预训练好的参数、原有的转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize model with the best available weights</span><br>weights = ResNet50_Weights.DEFAULT<br>model = resnet50(weights=weights)<br><span class="hljs-comment"># No weights - random initialization</span><br>model = resnet50(weights=<span class="hljs-literal">None</span>)<br><br><span class="hljs-comment"># model weight includes preprocessing transforms</span><br>preprocess = weights.transforms()<br><span class="hljs-comment"># Apply it to the input image</span><br>img_transformed = preprocess(img)<br></code></pre></td></tr></tbody></table></figure></li><li><p>可以对已经定义好的模型进行添加、修改 layer：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>vgg16.classifier.add_module(<span class="hljs-string">'add_linear'</span>, nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># 修改分类为10类</span><br>vgg16.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li></ul></li><li><p><code>Sequential</code>：将不同神经网络层进行顺序拼接，将整个容器视为单个模块，避免手动多次调用</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">self.net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>),<br>    nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>loss function</code>：</p><ul><li>Mean Squared Error（回归任务）：<code>criterion = nn.MSELoss()</code></li><li>Cross Entropy （分类任务）：<code>criterion = nn.CrossEntropyLoss()</code><ul><li>input 为 <span class="math inline">\((batch Size, class)\)</span>，对应每个 class 的输出概率</li><li>target 可以为 <span class="math inline">\((batch Size, 1)\)</span>，对应每个 sample 的 <code>class index</code>；也可以为 <span class="math inline">\((batch Size, class)\)</span>，对应类概率</li></ul></li><li>计算损失：<code>loss = criterion(model_output, expected_value)</code></li></ul></li></ul><h3 id="优化-optim">优化 | <a href="https://pytorch.org/docs/stable/optim.html">OPTIM</a></h3><p>实现各种优化算法的软件包。</p><ul><li><p>构造优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br>optimizer = optim.Adam([var1, var2], lr=<span class="hljs-number">0.0001</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>采取优化步骤：<code>optimizer.step()</code></p></li><li><p>结合训练过程的优化</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()<span class="hljs-comment"># 把上一个循环的梯度清0</span><br>    output = model(<span class="hljs-built_in">input</span>)<br>    loss = loss_fn(output, target)<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></tbody></table></figure></li><li><p>正则化：设置 <code>weight_decay &gt; 0</code> ，pytorch 自动完成正则化计算</p></li></ul><h3 id="神经网络训练-step">神经网络训练 | Step</h3><ul><li><p>定义：数据集、数据批次、模型、损失函数、优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = MyDataset(file)<span class="hljs-comment"># read data via MyDataset</span><br>tr_set = DataLoader(dataset, <span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)<span class="hljs-comment"># put dataset into Dataloader</span><br>model = MyModel().to(device)<span class="hljs-comment"># construct model and move to device </span><br>criterion = nn.MSELoss()<span class="hljs-comment"># set loss function</span><br>optimizer = torch.optim.SGD(model.parameters(), <span class="hljs-number">0.1</span>)<span class="hljs-comment"># set optimizer</span><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>) <span class="hljs-comment"># set gpu training</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>训练过程：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tensorboard</span><br>writer = SummaryWriter(<span class="hljs-string">"../logs"</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<span class="hljs-comment"># iterate n_epochs</span><br>model.train()<span class="hljs-comment"># set model to train mode</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> tr_set: <span class="hljs-comment"># iterate through the dataloader</span><br>optimizer.zero_grad()<span class="hljs-comment"># set gradient to zero</span><br>        x, y = x.to(device), y.to(device)<span class="hljs-comment"># move data to device (cpu/cuda)</span><br>        pred = model(x)<span class="hljs-comment"># forward pass (compute output)</span><br>        loss = criterion(pred, y)<span class="hljs-comment"># compute loss</span><br>        loss.backward()<span class="hljs-comment"># compute gradient (backpropagation)</span><br>        optimizer.step()<span class="hljs-comment"># update model with optimizer</span><br>    writer.add_scalar(<span class="hljs-string">"train_loss"</span>,loss.item(),epoch)<br></code></pre></td></tr></tbody></table></figure></li><li><p>验证过程：无需进行优化，只计算 <code>loss</code> 即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<span class="hljs-comment"># 开启验证模式</span><br>total_loss = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> dv_set:<br>    x, y = x.to(device), y.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<span class="hljs-comment"># 关闭梯度计算</span><br>        pred = model(x)<br>        loss = criterion(pred, y)<br>        total_loss += loss.cpu().item() * <span class="hljs-built_in">len</span>(x)<br>        avg_loss = total_loss / <span class="hljs-built_in">len</span>(dv_set.dataset)<br></code></pre></td></tr></tbody></table></figure></li><li><p>测试阶段：无需优化、计算损失，只需预测正确答案即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<br>preds = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tt_set:<br>    x = x.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        pred = model(x)<br>        preds.append(pred.cpu())<br></code></pre></td></tr></tbody></table></figure></li><li><p>存储模型</p><ul><li><p>Save：常用的方法是以字典的形式保存模型参数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">path = <span class="hljs-string">'model.pth'</span><br><span class="hljs-comment"># 存储模型训练好的参数、模型结构</span><br>torch.save(model, path)<br><span class="hljs-comment"># 存储模型训练好的参数</span><br>torch.save(model.state_dict(), path)<br></code></pre></td></tr></tbody></table></figure></li><li><p>Load：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模型训练好的参数、模型结构</span><br>model = torch.load(path)<br><span class="hljs-comment"># 加载模型训练好的参数</span><br>model = NeuralNetwork()<br>model.load_state_dict(ckpt)  <br></code></pre></td></tr></tbody></table></figure><blockquote><p>注意第一种方法：网络模型定义的代码要与 load 代码写在一起，或者 <code>from model import *</code>，这样才能成功加载</p></blockquote></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>保研小记</title>
    <link href="/Baoyan-essay.html"/>
    <url>/Baoyan-essay.html</url>
    
    <content type="html"><![CDATA[<h1 id="保研小记">保研小记</h1><div class="note note-info">            <p>一切都是最好的安排。</p>          </div><h2 id="写在前面">0. 写在前面</h2><p>大三，身处于人生的岔路口，无论是出国、工作、保研、考研的每一位同学，在答案没有正式揭晓前，都面临着选择与被选择。谨以此博客记录我曾经焦虑、一度感到非常煎熬的保研季，算是正式的说再见 <span class="github-emoji" style="display:inline;vertical-align:middle"><span>👋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>。</p><h2 id="个人情况">1. 个人情况</h2><ul><li><p>本科：普通中九计算机专业</p></li><li>排名：2%</li><li>获奖情况：一次国奖</li><li>竞赛经历：数学建模国一（自我认为竞赛方面除了 ACM 以外的奖项都不太能发挥太大用处。）</li><li>科研经历：无产出（这点真的在组面的时候很吃亏，一下就会被学术能力强的老师淘汰。）</li><li><p>最终去向：复旦大学，学硕</p></li></ul><h2 id="最初定位">2. 最初定位</h2><ul><li><p>院校选择：定位是华五，没敢奢求清北。心仪的院校是复旦、人大高瓴、自动化所。</p></li><li><p>老师选择：在组面的过程中确实也被磨平了心志，科研经历比不上有论文发表的大佬，觉得卷不过别人，只希望找到对应目标方向、人品较好的导师。</p></li><li>方向：AI 方向。</li><li><p>填报学位：没做好直博的准备，倾向学硕，专硕也可以接受（顾虑主要是专硕有些学校学费高且无宿舍）。</p></li></ul><h2 id="时间线">3. 时间线</h2><div class="note note-success">            <p>越早越好:)</p>          </div><ol type="1"><li>简历准备：大概是2月底3月初，后续也一直在更新简历的内容。<strong>简历上的每个点都要完全掌握。</strong></li><li>发邮件给老师：5月初才开始发了第一封（算是很晚的，有些学校完全是老师决定入营的）。<strong>3-4月联系意向导师完成project / 提前面试 → 院校夏令营报名前 → 入营后面试前 → 结果出来后</strong>，我觉得是都是联系老师的重要时间点，不用对老师没回邮件特别失落（老师收到的邮件确实很多，可以尝试再发一次）。</li><li>材料准备：5月中~6月初，差不多就是在夏令营填报过程中慢慢完善的</li><li>复习：6月，因为是AI方向，因此我大概复习的是数学基础课、数据结构和机器学习，准备英语问题。</li><li>夏令营正式开始： 7.1-7.24，这段是最辛苦的时候</li><li>保研间隙：8月，因为有了保底 offer 所以很纯粹地休息了一个月。后面深入了解了一下 offer 院校和导师的具体情况，还是觉得休息躺平早了…</li><li>预推免：9.13-9.15</li></ol><h2 id="夏令营情况">4. 夏令营情况</h2><p>入营大部分都是<strong>靠 rank 筛选</strong>，是否优营基本取决于面试，比较看重<strong>科研经历、机试能力</strong>，这些都需要提前的准备，丰富自己的简历。如果在本科期间，能与本校 / 外校导师实验室做科研并有所产出，在面试中取胜概率会大很多。</p><table><thead><tr class="header"><th>学校</th><th>学院/实验室名称</th><th>是否入营</th><th>结果</th></tr></thead><tbody><tr class="odd"><td>复旦</td><td>计算机学院，人工智能学硕</td><td>是</td><td>在 waiting list 中等位置，最终候补到（最终去向）</td></tr><tr class="even"><td>北大</td><td>软微专硕</td><td>是</td><td>优营</td></tr><tr class="odd"><td>北大</td><td>计算机学院，学硕</td><td>否</td><td>-</td></tr><tr class="even"><td>人大</td><td>高瓴，学硕</td><td>是</td><td>在 waiting list 中等偏后了</td></tr><tr class="odd"><td>国科大</td><td>自动化所，学硕</td><td>是</td><td>优营，无双选老师自动放弃</td></tr><tr class="even"><td>南大</td><td>人工智能学院，学硕</td><td>是</td><td>参加笔试通过了，面试时间冲突放弃。</td></tr><tr class="odd"><td>中科大</td><td>网安学院，学硕</td><td>是</td><td>优营，放弃</td></tr><tr class="even"><td>上交</td><td>电院，计算机方向学硕</td><td>是</td><td>时间冲突放弃</td></tr><tr class="odd"><td>国科大</td><td>软件所，学硕</td><td>否</td><td>-</td></tr></tbody></table><h3 id="复旦cs">复旦CS</h3><p>入营要求比较高，卡 rank 的，联系老师也应该没办法。300人入营，发放的名额非常少。入营后联络员通知拉进微信群，送了一件T恤和复旦大学的本子。</p><p>第一天是听各个方向的宣讲，感觉还挺重要的，有些老师会对自己实验室情况详细介绍，可以听下感兴趣的。晚上进行<strong>志愿的填报</strong>，这个很重要，算是大型<strong>博弈场</strong>，后续优营的评选、候补的情况完全都是在你<strong>填报的志愿队列排序</strong>。系统实时更新，可以看到当前报考人数/招生人数。附上一张最后的填报图，可以看出竞争真的很激烈。</p><p><img src="/img/post_img/志愿.png"></p><p>第二天上午是复旦的机考，2h3题。<strong>不设监考，不计入总分，但是面试会看到成绩，会问解题思路</strong>。OJ 系统评测，题目难度很大。下午是一个英语的面试流程，6-8min。问的范围蛮广的，需要提前准备。</p><p>第三天是专业面试，是最重要的，15min左右。先是两分钟的中文自我介绍。接下来问你的机试某道题的解题思路、时间复杂度、有没有更优解等。之后都问具体方向的问题，<strong>每个方向侧重问的内容不一</strong>，机器学习以及408均有可能。</p><h3 id="人大高瓴">人大高瓴</h3><p>入营要求高，也是卡 rank 的，学硕名额最终只有24个，比去年少了。整个流程可能为了杜绝作弊，保证公平性，步骤繁杂、有大量等候流程、非常浪费时间、教务老师也比较严格，参营体验真的不太好。</p><p>开营老师和学生都会来做介绍发言，高瓴有非常多年轻学术能力强的老师，值得一听。具体的考核签了比较严格的保密协议，无法透露，只能说是本人参与的<strong>难度最大</strong>的笔试和面试。</p><p>笔试：1.5h，<strong>官方发布的范围</strong>是数学（线性代数、概率论等）、程序设计、数据结构与算法（75分）。</p><p>面试：15min，同一组序号靠后的同学可能要等很久，并且等待过程中需要开着视频什么都不能做。<strong>官方发布的范围</strong>是数学、计算机、人工智能基础（75分）、 英语听、说及综合能力（50分）。</p><h3 id="自动化所">自动化所</h3><p>今年入营要求高，也是卡 rank 的。虽然是线上，但是真的能感受到学长学姐真正带动起群里的氛围，比较温馨，确实最像是一个夏令营的营。有连续几天的学术报告会和交流会，中间穿插一个心理测试，题量大，正常答就行。</p><p>只有面试，1人12min。先是自我介绍1min，简单英语问题。而后是数学概念题，<strong>高数、线代、概率论的基础概念</strong>很重要，不难，但是一定要提前复习。接下来都是围绕简历项目的内容展开。</p><p>优营率很高，貌似分为学硕 / 直博 / 专硕三个档次发放，感觉大部分参加的同学都拿到了Offer，但是必须选到老师才有效。后续需要「与老师双选签订协议」，这个我认为是自动化所真正的考核，热门导师的竞争比较激烈，每个老师只有1-2个名额（一般博导名额是博士，硕导名额是硕士）。需要 <strong>发邮件 → 课题组一起面试 → 确定是否双选 → 签协议</strong> 一系列过程，因此收到学院的 offer 远远没有结束考核。本人因为被几个课题组接连拒绝，比较心灰意冷，也不再另找老师签协议了。</p><h3 id="北大软微">北大软微</h3><p>一开始并不在目标院校的学院中，北大的报名材料是最多的，一向非常繁琐，需要另准备推荐信等。</p><p>变化比较大的是，一向以title、放养、就业导向闻名的软微学院，在开营招生会上宣布今年改成<strong>科研导向</strong>，学院说只给3-6个月实习，给的补助大部分能够 cover 3年9w的学费，无法知道后续具体政策落实如何。软微应该是靠 rank 初筛，然后需要选择对应方向的论文做汇报、面试。我填报的方向基本不考专业知识，只问了论文汇报、个人情况。面试完后可能会有导师电话联系，给<strong>非常短的时间</strong>让你确定来不来，一般和老师确定来的都能给 offer，优营率70%以上。这个阶段需要<strong>比较谨慎</strong>，因为软微大多数导师不做科研，老师之间差别比较大，可以直接深入询问老师是否放实习、培养方案、组里方向、科研or横向等。</p><ul><li><strong>优点：</strong>算是清北中门槛最低的一个院校，难度小。因为 pku 的 title，体制内有优势。往年因为放实习，就业率也非常好，是就业向同学非常好的选择。</li><li><strong>缺点：</strong>学费高，地处大兴，可能研二以后需要两头跑，宿舍不太方便住。科研向的同学能找到的老师非常少。</li></ul><h3 id="南大ai">南大AI</h3><p>相比起来，个人感觉教务老师很nice，应该是只允许入一个方向的营。因为时间冲突只参加了笔试。题量比较多，考了很多机器学习的知识，会先通过笔试筛掉一部分人，再确定面试的名单。</p><h3 id="中科大网安">中科大网安</h3><p>老师可能在多个学院有招生名额，除了学位外差别不大。因为联系的导师在网安招生，因此报了网安学院。点名表扬中科大，入营后发放了大礼包，真的诚意满满。中科大一向入营要求不高，优营也是发很多（毕竟科大因为在合肥，鸽子实在太多了），但是科大真的有很多实力强劲的导师，提前联系希望非常大。</p><p>无机试无笔试，为了公平，面试有两轮，流程一模一样，只是老师不一样。需要抽题号问专业知识，因为我本身没有学过 「网安」方向的任何知识，基本答不上来，联系的导师说没关系，可以回答没学过。</p><h2 id="预推免情况">5. 预推免情况</h2><p>因为夏令营有了 offer，一度也觉得比较满意，但是考虑到软微的总总不足，还是准备冲一下。因此预推免只报了清华大学深圳研究生院的考核。</p><h3 id="清深cs">清深CS</h3><p>机试：3题300分，题目是和本部贵系一样的，难度大。</p><p>综合面：共10分钟，读论文并翻译，老师问两个英文问题，然后中文问些心理等家常问题。</p><p>专业面：共10分钟，PPT自我介绍3-5分钟，老师围绕你的简历展开，可能很看你做的东西和老师方向会不会 match，本人和老师聊的完全不在一个频道上。</p><h2 id="总结">6. 总结</h2><h3 id="最终结果">最终结果</h3><p>夏令营留的一个 offer 只有软微。但是因为最开始想去的确实是复旦大学，并结合了往年鸽子较多的情况，在拿了 waiting list 后也询问了具体的排名，残留候补上的希望。清深结果9月19日上午出的，很意外居然拿到了offer，相比软微更倾向清深，因此也接受了预录取资格。没想到9月19日晚上复旦大学招生办打电话通知我候补上了，曾一度以为和上海无缘失去希望，最终也很惊喜，并且联系到学术好人品好的导师，算是圆梦了。在当天晚上就释放了清深的名额，第二天也和软微的老师说明了情况，老师表示理解。我也不喜欢这样的鸽子行为，深感抱歉，只能尽可能降低对老师们的影响了。</p><p>至此，整个持续好几个月的保研应该完整地结束了，无论是院校还是老师，我都觉得非常满意，算是一个圆满的句号。是很艰难的过程，一路上会看到各方面实力都碾压自己的大佬，一度非常质疑自己，被老师拒绝到怀疑人生，时间安排紧复习来不及等问题，最终还是熬过来了。真心感谢一路帮助我的老师、学长学姐，以及一起参加保研的小伙伴~</p><p>关于<strong>「鸽子与海王」</strong>的现象，恶意收集 offer，等到最后再鸽真的很不好。个人认为手上有2个 offer 即可，如果拿到的1个 offer 不满意，可以作为保底再报一个冲一冲，但是真的需要尽早释放，邮件和招生办的老师说明不去、请求老师的谅解等等。</p><h3 id="保研心得">保研心得</h3><div class="note note-success">            <p>华五强导-&gt;要求高-&gt;大佬才能拿到offer-&gt;大佬大概率能去清北-&gt;鸽-&gt;有机会补录上</p>          </div><p>最后的最后，送上一份网上流传很广的某学校招生老师的鸡汤共勉，</p><blockquote><p>在每位同学漫长的人生里，会经历很多这样的时刻，我们有时做自己的分子，有时做别人的分母。但无论如何，都需要以做分母的心态，尽做分子的努力。</p><p>从研究方法的角度来讲，我们抽取了一个很小样本：从诸位21年的生活里面抽取了几天，从你所有的知识库里面抽取了几个问题，从你无数侧面里抽取了一面，这里没法测量你的curiosity，personality等等宝贵的品质。</p><p>我们只能在此公布一个结果，但任何一个结果都不能最终定义你。</p><p>能定义你的，只有你自己。</p></blockquote><p>以及，一切都是最好的安排，顺其自然就好。上岸不是终点，保持一以贯之的努力，每天的微小积累才是决定漫漫人生路的因素~</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello My Blog Space</title>
    <link href="/hello-world.html"/>
    <url>/hello-world.html</url>
    
    <content type="html"><![CDATA[<p>本博客参考 <a href="https://hwcoder.top/Hello-My-World#搭建历程">Hello My World | Hwcoder</a> 的搭建过程以及 <a href="https://hexo.fluid-dev.com/docs/guide/#关于指南">Hexo Fluid 配置指南</a> 下完成。主要基于 Hexo + GitHub 搭建，采用 Fluid 主题。</p><h2 id="搭建历程">搭建历程</h2><h2 id="hexo-命令">hexo 命令</h2><h4 id="新建文章">新建文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo n <span class="hljs-string">"My New Post"</span> <span class="hljs-comment"># new</span><br></code></pre></td></tr></tbody></table></figure><h4 id="清除本地缓存">清除本地缓存</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo cl  <span class="hljs-comment">#clean</span><br></code></pre></td></tr></tbody></table></figure><h4 id="生成静态文章">生成静态文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo g <span class="hljs-comment"># generate</span><br></code></pre></td></tr></tbody></table></figure><h4 id="阅览文章">阅览文章</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s <span class="hljs-comment"># show</span><br></code></pre></td></tr></tbody></table></figure><h4 id="部署推送到github">部署推送到github</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo d <span class="hljs-comment"># deploy</span><br></code></pre></td></tr></tbody></table></figure><h2 id="评论功能">评论功能</h2><p>Valine 是国内的一款极简风格的评论软件，也是 Fluid 支持的评论软件之一。在 <code>comment</code> 中选择 <code>valine</code>，之后找到相应的配置区域进行如下操作：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">comments:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">valine</span><br></code></pre></td></tr></tbody></table></figure><p>进入官网 <a href="https://leancloud.cn/">LeanCloud</a> 完成注册，然后在控制台创建一个项目 <code>Blog.Comments</code> 后，通过 <strong>控制台 &gt; 设置 &gt; 应用凭证</strong> 获取密钥（App ID 和 App Key），在对应位置填入。</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">valine:</span><br>  <span class="hljs-attr">appId:</span> <br>  <span class="hljs-attr">appKey:</span> <br></code></pre></td></tr></tbody></table></figure><h3 id="latex-数学公式">Latex 数学公式</h3><p>主要方法依照参考博客，但是在执行 <code>hexo cl</code> 、<code>hexo g</code> 可能会出现报错 <code>pandoc exited with code 9: pandoc: Unknown extension: smart</code>。</p><p>解决办法为将 <code>node_modules\hexo-renderer-pandoc\index.js</code> 中的</p><figure class="highlight js"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> args = [ <span class="hljs-string">'-f'</span>, <span class="hljs-string">'markdown-smart'</span>+extensions, <span class="hljs-string">'-t'</span>, <span class="hljs-string">'html-smart'</span>, math]<br></code></pre></td></tr></tbody></table></figure><p>改成</p><figure class="highlight js"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">var</span> args = [ <span class="hljs-string">'-f'</span>, <span class="hljs-string">'markdown'</span>+extensions, <span class="hljs-string">'-t'</span>, <span class="hljs-string">'html'</span>, math]<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术栈</category>
      
      <category>博客</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
