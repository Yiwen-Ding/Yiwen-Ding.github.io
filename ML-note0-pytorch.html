<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="code-dWHnANgGFW"><link rel="apple-touch-icon" sizes="76x76" href="/img/about_me.jpg"><link rel="icon" href="/img/about_me.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="CindyWen"><meta name="keywords" content=""><meta name="description" content="本文是结合官方文档，各个开源视频的 PyTorch 学习总结。"><meta property="og:type" content="article"><meta property="og:title" content="ML学习笔记 #00 PyTorch"><meta property="og:url" content="https://yiwen-ding.github.io/ML-note0-pytorch"><meta property="og:site_name" content="CindyWen"><meta property="og:description" content="本文是结合官方文档，各个开源视频的 PyTorch 学习总结。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://yiwen-ding.github.io/img/blog/ML-note0-pytorch/PyTorch.png"><meta property="article:published_time" content="2022-11-23T12:56:45.000Z"><meta property="article:modified_time" content="2022-11-23T13:46:37.420Z"><meta property="article:author" content="CindyWen"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://yiwen-ding.github.io/img/blog/ML-note0-pytorch/PyTorch.png"><title>ML学习笔记 #00 PyTorch - CindyWen</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"yiwen-ding.github.io",root:"/",version:"1.9.3",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>CindyWen&#39;s blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/post_bg.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">ML学习笔记 #00 PyTorch</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-11-23 20:56" pubdate>2022年11月23日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 6.5k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 55 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">ML学习笔记 #00 PyTorch</h1><div class="markdown-body"><p>本系列为 Machine Learning 学习笔记，主要记录跟随李宏毅老师 <a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">ML 2022 Spring</a> 的学习收获。本文是通过官方快速入门 <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/index.html">PyTorch Tutorials</a>、官方 API 文档 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">PyTorch documentation</a>、<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/">b站学习视频</a>、<a target="_blank" rel="noopener" href="https://youtu.be/85uJ9hSaXig">李宏毅老师机器学习课程</a> 的 PyTorch 学习总结。</p><h2 id="tensor"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">Tensor</a></h2><p>类似于 <code>ndarray</code> 的数据结构，构建多维矩阵，可以在 GPU 上训练，并且支持自动微分。</p><ul><li><p>初始化</p><ul><li><p>直接数字构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])					<span class="hljs-comment"># directly from data</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>从 NumPy array 转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.from_numpy(np.array([[<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]))		<br></code></pre></td></tr></tbody></table></figure></li><li><p>从另一个 tensor 中构造</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># datatype 为float</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>特殊构造0、1、随机 tensor</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>主要属性（shape、datatype、device）</p></li><li><p>操作方法：</p><ul><li><p>转移到 GPU 训练：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>	tensor = tensor.to(<span class="hljs-string">'cuda'</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>切片操作：类似 <code>numpy</code></p></li><li><p>矩阵堆叠与拼接</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)		<span class="hljs-comment"># dim = 1，列上相接</span><br>t1 = torch.stack([tensor, tensor, tensor], dim=<span class="hljs-number">0</span>)	<span class="hljs-comment"># dim = 0，行上相接</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>乘法运算</p><ul><li><p><code>element-wise product</code> 对应位置的数字相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t2 = tensor.mul(tensor) <br>t2 = tensor * tensor<br></code></pre></td></tr></tbody></table></figure></li><li><p>矩阵相乘</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">t3 = tensor.matmul(tensor.T)<br>t3 = tensor @ tensor.T<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>压缩与扩张</p><ul><li><p><code>torch.squeeze()</code>：删除矩阵中大小为1的所有维度</p><p>例如输入 <span class="math inline">\(A \times 1 \times B \times C \times 1 \times D\)</span>，转换为 <span class="math inline">\(A \times B \times C \times D\)</span></p></li><li><p><code>torch.unsqueeze()</code>：在指定位置插入维度为的张量</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>torch.unsqueeze(x, <span class="hljs-number">0</span>)<br>torch.unsqueeze(x, <span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>沿着某维度复制张量 <code>repeat</code></p><ul><li><p>参数是对应维度的复制个数，上段代码为0维复制两次，1维复制两次，则得到以上运行结果。其余扩展情况依此类推</p></li><li><p>repeat参数个数与 tensor 维数一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(b.shape)		<span class="hljs-comment"># 得到结果torch.Size([4, 6])</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>repeat参数个数与tensor维数不一致时</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([1, 4, 3])</span><br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># a形状(2,3)</span><br>a = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                  [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-comment"># repeat参数比维度多，在扩展前先讲a的形状扩展为(1,2,3)然后复制</span><br>b = a.repeat(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(b.shape)  <span class="hljs-comment"># 得到结果torch.Size([2, 2, 3])</span><br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p><strong>就地操作 In-place operations：</strong></p><ul><li>操作影响 被操作 tensor 本身的值发生变化，即修改内存</li><li>可以节省内存，但在计算导数可能会出现问题，不推荐使用</li></ul><figure class="highlight apache"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">tensor</span>.add_(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li><li><p>自动求导：<code>torch.autograd</code></p><ul><li><p>支持自动微分：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>], [-<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]], requires_grad=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>backward</code>：计算给定张量相对于自变量的梯度总和</p></li><li><p><code>grad</code>：相对于输入的输出梯度的总和。</p></li></ul><p><img src="img/blog/ML-note0-pytorch/image-20221101204244079.png" srcset="/img/loading.gif" lazyload alt="image-20221101204244079" style="zoom:50%"></p></li></ul><h2 id="数据加载-datasets-dataloaders">数据加载 | <a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></h2><h3 id="dataset">dataset</h3><ul><li><p>获取和加载数据集</p><ul><li>PyTorch 中提供了许多预加载的数据集（如 FashionMNIST），通过调用函数即可加载</li><li><code>torchvision</code> 库：图片数据集 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">Image Datasets</a></li><li><code>torchtext</code> 库：文本数据集 <a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/datasets.html">Text Datasets</a></li><li><code>torchaudio</code> 库：音频信号数据集 <a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,	<span class="hljs-comment"># root是存储训练/测试数据的路径，</span><br>    train=<span class="hljs-literal">True</span>,		<span class="hljs-comment"># train指定训练或测试数据集，</span><br>    download=<span class="hljs-literal">True</span>,	<span class="hljs-comment"># 下载数据</span><br>    transform=ToTensor()	<span class="hljs-comment"># feature and label transformations</span><br>)<br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">"data"</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>创建自定义数据集，继承 <code>Dataset</code> 类，重写 <code>init</code>、<code>getitem</code>、<code>len</code> 方法</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, file</span>):<br>		self.data = ...<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>		<span class="hljs-keyword">return</span> self.data[index]<br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>		<span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br></code></pre></td></tr></tbody></table></figure></li></ul><h3 id="dataloader">dataloader</h3><ul><li><p>进行数据加载的功能，数据集分为“小批量”，是否需要乱序打乱</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>迭代每个元素</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br></code></pre></td></tr></tbody></table></figure></li></ul><figure><img src="./img/blog/ML-note0-pytorch/image-20221031223432180.png" srcset="/img/loading.gif" lazyload alt="image-20221031223432180"><figcaption>image-20221031223432180</figcaption></figure><h2 id="tensorboard-可视化"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensorboard.html">TensorBoard</a> | 可视化</h2><ul><li><p>可视化数据</p><ul><li><code>add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=‘CHW’)</code>：绘制图片，可用于检查模型的输入，监测 feature map 的变化，或是观察 weight。</li></ul></li><li>可视化模型内部的 layer<ul><li><code>add_graph(model, input_to_model=None, verbose=False, use_strict_trace=True)</code>：每个 layer 的输入、输出维度</li></ul></li><li>可视化神经网络模型训练过程、结果。<ul><li><code>add_scalars(tag, scalar_value, global_step=None)</code></li></ul></li></ul><h2 id="transforms"><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html">Transforms</a></h2><ul><li><p>作用：对图像进行归一化 <code>Normalize</code>、旋转 <code>rotate</code>、裁剪 <code>resize</code>、灰度等</p></li><li><p>输入：<code>PIL</code> 库的方法 <code>Image.open()</code>；<code>opencv</code> 的方法 <code>cv.imread()</code> 输入图像</p></li><li><p><code>ToTensor()</code></p><p>将 PIL 图像或 NumPyinto 转换为 <code>tensor</code>，并缩放图像的像素强度值在 [0.， 1.] 范围内。</p></li><li><p>将所有变换组合在一起</p></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">transforms.Compose([<br>     transforms.CenterCrop(<span class="hljs-number">10</span>),<br>     transforms.PILToTensor(),<br>     transforms.ConvertImageDtype(torch.<span class="hljs-built_in">float</span>),<br>])<br></code></pre></td></tr></tbody></table></figure><h2 id="神经网络">神经网络</h2><h3 id="nn"><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html">NN</a></h3><ul><li><p><code>module</code>：构建所有神经网络的基类，需要进行继承</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)	<span class="hljs-comment"># 模型和层的初始化</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.conv1(x))<br>        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))		<span class="hljs-comment"># 计算输出</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>已实现的模型直接调用：</p><ul><li><p><a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models.html">Models and pre-trained weights — Torchvision</a>：包括 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models/alexnet.html">AlexNet</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/models/vgg.html">VGG</a> 等</p><ul><li><p>可以选择是否使用预训练好的参数、原有的转换</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize model with the best available weights</span><br>weights = ResNet50_Weights.DEFAULT<br>model = resnet50(weights=weights)<br><span class="hljs-comment"># No weights - random initialization</span><br>model = resnet50(weights=<span class="hljs-literal">None</span>)<br><br><span class="hljs-comment"># model weight includes preprocessing transforms</span><br>preprocess = weights.transforms()<br><span class="hljs-comment"># Apply it to the input image</span><br>img_transformed = preprocess(img)<br></code></pre></td></tr></tbody></table></figure></li><li><p>可以对已经定义好的模型进行添加、修改 layer：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>vgg16.classifier.add_module(<span class="hljs-string">'add_linear'</span>, nn.Linear(<span class="hljs-number">1000</span>,<span class="hljs-number">10</span>))<br><span class="hljs-comment"># 修改分类为10类</span><br>vgg16.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></tbody></table></figure></li></ul></li></ul></li><li><p><code>Sequential</code>：将不同神经网络层进行顺序拼接，将整个容器视为单个模块，避免手动多次调用</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">self.net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>),<br>    nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>)<br></code></pre></td></tr></tbody></table></figure></li><li><p><code>loss function</code>：</p><ul><li>Mean Squared Error（回归任务）：<code>criterion = nn.MSELoss()</code></li><li>Cross Entropy （分类任务）：<code>criterion = nn.CrossEntropyLoss()</code><ul><li>input 为 <span class="math inline">\((batch Size, class)\)</span>，对应每个 class 的输出概率</li><li>target 可以为 <span class="math inline">\((batch Size, 1)\)</span>，对应每个 sample 的 <code>class index</code>；也可以为 <span class="math inline">\((batch Size, class)\)</span>，对应类概率</li></ul></li><li>计算损失：<code>loss = criterion(model_output, expected_value)</code></li></ul></li></ul><h3 id="优化-optim">优化 | <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html">OPTIM</a></h3><p>实现各种优化算法的软件包。</p><ul><li><p>构造优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br>optimizer = optim.Adam([var1, var2], lr=<span class="hljs-number">0.0001</span>)<br></code></pre></td></tr></tbody></table></figure></li><li><p>采取优化步骤：<code>optimizer.step()</code></p></li><li><p>结合训练过程的优化</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, target <span class="hljs-keyword">in</span> dataset:<br>    optimizer.zero_grad()		<span class="hljs-comment"># 把上一个循环的梯度清0</span><br>    output = model(<span class="hljs-built_in">input</span>)<br>    loss = loss_fn(output, target)<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></tbody></table></figure></li><li><p>正则化：设置 <code>weight_decay &gt; 0</code> ，pytorch 自动完成正则化计算</p></li></ul><h3 id="神经网络训练-step">神经网络训练 | Step</h3><ul><li><p>定义：数据集、数据批次、模型、损失函数、优化器</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">dataset = MyDataset(file)								<span class="hljs-comment"># read data via MyDataset</span><br>tr_set = DataLoader(dataset, <span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>)			<span class="hljs-comment"># put dataset into Dataloader</span><br>model = MyModel().to(device)							<span class="hljs-comment"># construct model and move to device </span><br>criterion = nn.MSELoss()								<span class="hljs-comment"># set loss function</span><br>optimizer = torch.optim.SGD(model.parameters(), <span class="hljs-number">0.1</span>)	<span class="hljs-comment"># set optimizer</span><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>) <span class="hljs-comment"># set gpu training</span><br></code></pre></td></tr></tbody></table></figure></li><li><p>训练过程：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># tensorboard</span><br>writer = SummaryWriter(<span class="hljs-string">"../logs"</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):				<span class="hljs-comment"># iterate n_epochs</span><br>	model.train()							<span class="hljs-comment"># set model to train mode					</span><br>	<span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> tr_set: 					<span class="hljs-comment"># iterate through the dataloader</span><br>		optimizer.zero_grad()				<span class="hljs-comment"># set gradient to zero</span><br>        x, y = x.to(device), y.to(device)	<span class="hljs-comment"># move data to device (cpu/cuda)</span><br>        pred = model(x)						<span class="hljs-comment"># forward pass (compute output)</span><br>        loss = criterion(pred, y)			<span class="hljs-comment"># compute loss</span><br>        loss.backward()						<span class="hljs-comment"># compute gradient (backpropagation)</span><br>        optimizer.step()					<span class="hljs-comment"># update model with optimizer</span><br>    writer.add_scalar(<span class="hljs-string">"train_loss"</span>,loss.item(),epoch)<br></code></pre></td></tr></tbody></table></figure></li><li><p>验证过程：无需进行优化，只计算 <code>loss</code> 即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()								<span class="hljs-comment"># 开启验证模式</span><br>total_loss = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> dv_set:<br>    x, y = x.to(device), y.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():					<span class="hljs-comment"># 关闭梯度计算</span><br>        pred = model(x)<br>        loss = criterion(pred, y)<br>        total_loss += loss.cpu().item() * <span class="hljs-built_in">len</span>(x)<br>        avg_loss = total_loss / <span class="hljs-built_in">len</span>(dv_set.dataset)<br></code></pre></td></tr></tbody></table></figure></li><li><p>测试阶段：无需优化、计算损失，只需预测正确答案即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()<br>preds = []<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tt_set:<br>    x = x.to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        pred = model(x)<br>        preds.append(pred.cpu())<br></code></pre></td></tr></tbody></table></figure></li><li><p>存储模型</p><ul><li><p>Save：常用的方法是以字典的形式保存模型参数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">path = <span class="hljs-string">'model.pth'</span><br><span class="hljs-comment"># 存储模型训练好的参数、模型结构</span><br>torch.save(model, path)<br><span class="hljs-comment"># 存储模型训练好的参数</span><br>torch.save(model.state_dict(), path)<br></code></pre></td></tr></tbody></table></figure></li><li><p>Load：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模型训练好的参数、模型结构</span><br>model = torch.load(path)<br><span class="hljs-comment"># 加载模型训练好的参数</span><br>model = NeuralNetwork()<br>model.load_state_dict(ckpt)  <br></code></pre></td></tr></tbody></table></figure><blockquote><p>注意第一种方法：网络模型定义的代码要与 load 代码写在一起，或者 <code>from model import *</code>，这样才能成功加载</p></blockquote></li></ul></li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>ML学习笔记 #00 PyTorch</div><div>https://yiwen-ding.github.io/ML-note0-pytorch</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>CindyWen</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年11月23日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/Baoyan-essay" title="保研小记"><span class="hidden-mobile">保研小记</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"7Ga9gCkoL9V2jdNQgXHdzud4-gzGzoHsz","appKey":"FGdWtMdPhO4fjMsxnDlriMuM","path":"window.location.pathname","placeholder":"欢迎评论:)","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>