<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><meta name="baidu-site-verification" content="code-dWHnANgGFW"><link rel="apple-touch-icon" sizes="76x76" href="/img/about_me.jpg"><link rel="icon" href="/img/about_me.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="CindyWen"><meta name="keywords" content=""><meta name="description" content="本文是卷积神经网络与循环网络的学习笔记。"><meta property="og:type" content="article"><meta property="og:title" content="ML学习笔记 #01 CNN &amp; RNN"><meta property="og:url" content="https://yiwen-ding.github.io/ML-note1-cnn-rnn"><meta property="og:site_name" content="CindyWen"><meta property="og:description" content="本文是卷积神经网络与循环网络的学习笔记。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://yiwen-ding.github.io/img/blog/ML-note1-cnn-rnn/cnn.png"><meta property="article:published_time" content="2022-11-29T12:07:39.000Z"><meta property="article:modified_time" content="2022-11-29T12:25:44.492Z"><meta property="article:author" content="CindyWen"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://yiwen-ding.github.io/img/blog/ML-note1-cnn-rnn/cnn.png"><title>ML学习笔记 #01 CNN &amp; RNN - CindyWen</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"yiwen-ding.github.io",root:"/",version:"1.9.3",typing:{enable:!1,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:3},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml"};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 6.3.0"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>CindyWen&#39;s blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/post_bg.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle">ML学习笔记 #01 CNN & RNN</span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-11-29 20:07" pubdate>2022年11月29日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 40 分钟</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar category-bar" style="margin-right:-1rem"><div class="category-list"></div></aside></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">ML学习笔记 #01 CNN &amp; RNN</h1><div class="markdown-body"><h2 id="卷积神经网络-convolution-neural-network">1 卷积神经网络 | Convolution Neural Network</h2><h3 id="why-cnn">1.1 why cnn?</h3><p>一张图片是三维的矩阵，对应的维度为 <span class="math inline">\([width, col, channel]\)</span>。将三维的矩阵 <code>flatten</code> 为 channel 个 <span class="math inline">\(width \times col\)</span> 的长向量，并输入神经网络。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221105225305450.png" srcset="/img/loading.gif" lazyload alt="image-20221105225305450" style="zoom:40%"></p><p>这样的做法存在着以下缺点：</p><ul><li><p>全连接参数多</p><p>在全连接前馈神经网络中, 如果第 <span class="math inline">\(l\)</span> 层有 <span class="math inline">\(M_{l}\)</span> 个神经元，第 <span class="math inline">\(l-1\)</span> 层有 <span class="math inline">\(M_{l-1}\)</span> 个 神经元，连接边有 <span class="math inline">\(M_{l} \times M_{l-1}\)</span> 个，权重矩阵有 <span class="math inline">\(M_{l} \times M_{l-1}\)</span> 个参数。当 <span class="math inline">\(M_{l}\)</span> 和 <span class="math inline">\(M_{l-1}\)</span> 都很大时，权重参数非常多，训练效率降低，易出现过拟合。</p></li><li><p>局部不变性</p><p>自然图像中的物体都具有<strong>局部性特征</strong>（如图中猫的耳朵、眼睛，对应物体分类而言是重要的特征）， 而全连接前馈网络很难提取。</p></li></ul><h3 id="结构-structure">1.2 结构 | Structure</h3><p>CNN 并非是对旋转、放大不变的，因此它需要数据增强。</p><h4 id="感受野-receptive-field">感受野 | Receptive field</h4><ul><li><p>stride：感受野步长大小 <span class="math inline">\(S\)</span></p></li><li><p>padding：填充大小 <span class="math inline">\(P\)</span></p></li><li><p>kernel size：感受野大小 <span class="math inline">\(W \times W\)</span></p></li></ul><h4 id="卷积层-convolution">卷积层 | Convolution</h4><ul><li>从神经网络的角度理解：<ul><li>不同的感受野共享参数</li></ul></li><li>从 Filter 的角度理解：<ul><li>使用 Filter 扫整张图片，提取一个局部区域的特征， 不同的卷积核相当于不同的特征提取器</li></ul></li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221106230228014.png" srcset="/img/loading.gif" lazyload alt="image-20221106230228014" style="zoom:50%"></p><ul><li>卷积层输出大小：<span class="math inline">\(N = (W − F + 2P)/S + 1\)</span></li></ul><h4 id="池化层-pooling">池化层 | Pooling</h4><ul><li><p>特点：无需学习的参数</p></li><li><p>作用</p><p>进行特征选择，降低特征数量，从而减少参数数量。</p></li><li><p>原因</p><p>卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的<strong>神经元个数</strong>并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合、计算复杂度高。 在卷积层之后加上一个汇聚层，从而降低特征维数，避免过拟合。</p></li><li><p>池化函数</p><ul><li>Max Pooling：最大池化，选出最显著的像素</li><li>Mean Pooling：平均池化</li></ul></li></ul><h3 id="图片转换-image-transformation">1.3 图片转换 | Image Transformation</h3><p>平移、旋转、缩放都是在原始对图像矩阵上进行运算。</p><ul><li><p>缩放 <span class="math display">\[ \left[\begin{array}{l} x^{\prime} \\ y^{\prime} \end{array}\right]=\left[\begin{array}{cc} a &amp; 0 \\ 0 &amp; d \end{array}\right]\left[\begin{array}{l} x \\ y \end{array}\right]+\left[\begin{array}{l} 0 \\ 0 \end{array}\right] \]</span></p></li><li><p>旋转，旋转 <span class="math inline">\(\theta\)</span> ° <span class="math display">\[ \left[\begin{array}{l} x^{\prime} \\ y^{\prime} \end{array}\right]=\left[\begin{array}{cc} \cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{array}\right]\left[\begin{array}{l} x \\ y \end{array}\right]+\left[\begin{array}{l} 0 \\ 0 \end{array}\right] \]</span></p></li></ul><h2 id="循环神经网络-rnn">2 循环神经网络 | RNN</h2><p>在许多现实任务中，网络的输出不仅和当前时刻的输入相关， 也和其过去一段时间的输出相关。例如一个有限状态自动机， 其下一个时刻的状态（ 输出） 不仅仅和当前输入相关， 也和当前状态（ 上一个时刻的输出） 相关。</p><p>为了处理时序数据并利用其历史信息， 需要让网络具有短期记忆能力。而前馈网络是一种静态网络， 不具备这种记忆能力，因此提出了循环神经网络 RNN。</p><h3 id="简单循环网络-simple-recurrent-network">2.1 简单循环网络 | Simple Recurrent Network</h3><ul><li><p>有特定的 memory 存储上一个时刻的状态值</p><p>令向量 <span class="math inline">\(\boldsymbol{x}_t \in \mathbb{R}^M\)</span> 表示在时刻 <span class="math inline">\(t\)</span> 时网络的输入， <span class="math inline">\(\boldsymbol{h}_t \in \mathbb{R}^D\)</span> 表示隐藏层状态值，则 <span class="math inline">\(\boldsymbol{h}_t\)</span> 不仅和当前时刻的输入 <span class="math inline">\(\boldsymbol{x}_t\)</span> 相关, 也和上一个时刻的 隐藏层状态 <span class="math inline">\(\boldsymbol{h}_{t-1}\)</span> 相关，更新公式为 <span class="math display">\[ \boldsymbol{z}_t=\boldsymbol{U} \boldsymbol{h}_{t-1}+\boldsymbol{W} \boldsymbol{x}_t+\boldsymbol{b} \]</span></p><p><span class="math display">\[ \boldsymbol{h}_t=f\left(\boldsymbol{z}_t\right) \]</span></p></li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221108213915842.png" srcset="/img/loading.gif" lazyload alt="image-20221108213915842" style="zoom:33%"></p><ul><li><p>缺点：RNN 每个时刻计算结果后乘 <span class="math inline">\(w\)</span> 作为下一刻的输入，导致最后一次计算的结果形式上是 <span class="math inline">\(w\)</span>​ 累乘。并使用非线性激活函数为 Logistic 函数或 Tanh 函数作为非线性激活函数， 其导数值都小于 1。在建模<strong>长时间间隔（ Long Range）</strong> 的状态之间的依赖关系时，会出现 <strong>梯度消失（gradient vanishing）</strong> 和 gradient explode 的问题。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221109162512582.png" srcset="/img/loading.gif" lazyload alt="image-20221109162512582" style="zoom:33%"></p></li></ul><h3 id="长短期记忆网络-lstm">2.2 长短期记忆网络 | LSTM</h3><p>为了改善循环神经网络的长程依赖问题， 引入门控机制来控制信息的累积速度， 包括有选择地加入新的信息， 并有选择地遗忘之前累积的信息。</p><p>因此，长短期记忆网络（ Long Short-Term Memory Network， LSTM）被提出，主要改进点有两个方面：</p><ul><li>新的内部状态： LSTM网络引入新的内部状态 <span class="math inline">\(C_t\)</span> 专门进行线性的循环信息传递， 同时（ 非线性地） 输出信息给隐藏层的外部状态 <span class="math inline">\(h_t\)</span> ， <span class="math inline">\(C_t\)</span> 计算公式如下：<br><span class="math display">\[ \begin{aligned} \boldsymbol{C}_t &amp;=\boldsymbol{f}_t \odot \boldsymbol{C}_{t-1}+\boldsymbol{i}_t \odot \tilde{\boldsymbol{C}}_t \\ \boldsymbol{h}_t &amp;=\boldsymbol{o}_t \odot \tanh \left(\boldsymbol{C}_t\right) \end{aligned} \]</span></li></ul><p>其中 $$ 表示向量元素乘积，<span class="math inline">\(\tilde{\boldsymbol{C}}_t\)</span> 是通过非线性函数得到<strong>新的候选状态</strong>，即 new memory <span class="math display">\[ \tilde{\boldsymbol{C}}_t = tanh(\boldsymbol{W}_{c}\boldsymbol{x}_t + \boldsymbol{U}_{c}\boldsymbol{h}_{t-1} + \boldsymbol{b}_{c}) \]</span> <img src="img/blog/ML-note1-cnn-rnn/image-20221109195443966.png" srcset="/img/loading.gif" lazyload alt="image-20221109195443966"></p><ul><li><p>门控机制： 门（ gate） 为一个二值变量{0, 1}， 0代表关闭状态， 不许任何信息通过； 1代表开放状态， 允许所有信息通过。LSTM 引入三个“门” 分别为输入门 <span class="math inline">\(\boldsymbol{i}_{t}\)</span>​、遗忘门 <span class="math inline">\(\boldsymbol{f}_{t}\)</span>​ 和输出门 <span class="math inline">\(\boldsymbol{o}_{t}\)</span>。这三个门 是一种<strong>“软” 门</strong>， 取值在 (0, 1) 之间，以<strong>一定的比例</strong>允许信息通过 。</p><ul><li><strong>遗忘门</strong> <span class="math inline">\(\boldsymbol{f}_{t}\)</span> 控制上一个时刻的<strong>内部状态</strong> <span class="math inline">\(\boldsymbol{C}_{t-1}\)</span> 需要<strong>遗忘</strong>多少信息</li><li><strong>输入门</strong> <span class="math inline">\(\boldsymbol{i}_{t}\)</span> 控制当前时刻的<strong>候选状态</strong> $_t $ 有多少信息需要<strong>保存</strong></li><li><strong>输出门</strong> <span class="math inline">\(\boldsymbol{o}_{t}\)</span> 控制当前时刻的<strong>内部状态</strong> <span class="math inline">\(\boldsymbol{C}_{t}\)</span> 有多少信息需要输出给<strong>外部状态</strong> $_t $</li></ul><p><span class="math display">\[ \begin{aligned} \boldsymbol{i}_t &amp;=\sigma\left(\boldsymbol{W}_i \boldsymbol{x}_t+\boldsymbol{U}_i \boldsymbol{h}_{t-1}+\boldsymbol{b}_i\right), \\ \boldsymbol{f}_t &amp;=\sigma\left(\boldsymbol{W}_f \boldsymbol{x}_t+\boldsymbol{U}_f \boldsymbol{h}_{t-1}+\boldsymbol{b}_f\right) \\ \boldsymbol{o}_t &amp;=\sigma\left(\boldsymbol{W}_o \boldsymbol{x}_t+\boldsymbol{U}_o \boldsymbol{h}_{t-1}+\boldsymbol{b}_o\right) \end{aligned} \]</span></p></li></ul><blockquote><p>注意：<span class="math inline">\(\boldsymbol{W}_*\)</span>、<span class="math inline">\(\boldsymbol{U}_*\)</span>、<span class="math inline">\(\boldsymbol{b}_*\)</span>均为可学习的网络参数，也可以将 <span class="math inline">\(\boldsymbol{x}_t\)</span>、<span class="math inline">\(\boldsymbol{h}_{t-1}\)</span> 拼接在一起，共享参数 <span class="math inline">\(\boldsymbol{W}_*\)</span></p></blockquote><p><img src="img/blog/ML-note1-cnn-rnn/image-20221108215916637.png" srcset="/img/loading.gif" lazyload alt="image-20221108215916637" style="zoom:33%"></p><ul><li><p>LSTM是涉及memory 和 input 状态信息的累加，一定程度上解决了 gradient vanishing 问题。</p></li><li><p>与简单循环网络的区别</p><ul><li>简单循环网络中的隐状态 <span class="math inline">\(\boldsymbol{h}\)</span> 存储了历史信息， 每个时刻都会被重写， 是一种短期记忆。</li><li>LSTM 中, <span class="math inline">\(\boldsymbol{h}\)</span> 看作网络参数， 隐含了从训练数据中学到的经验， 其更新周期要远远慢于短期记忆。记忆单元 <span class="math inline">\(\boldsymbol{C}\)</span> 可以在某个时刻捕捉到某个关键信息，并将此关键信息保存一定的时间间隔生命。</li></ul><p><img src="img/blog/ML-note1-cnn-rnn/image-20221109165204933.png" srcset="/img/loading.gif" lazyload alt="image-20221109165204933" style="zoom:67%"></p></li></ul><h3 id="gru-门控循环单元">2.3 GRU | 门控循环单元</h3><p>GRU（Gate Recurrent Unit）是 RNN 的一种。和 LSTM 一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的，相比之下<strong>参数量少</strong>，更容易进行训练，提高训练效率。</p><p><img src="img/blog/ML-note1-cnn-rnn/image-20221123193947597.png" srcset="/img/loading.gif" lazyload alt="image-20221123193947597" style="zoom:80%"></p><ul><li><strong>模型输入：</strong>上一个时刻传输的状态 <span class="math inline">\(h_{t−1}\)</span> 和当前节点的输入 <span class="math inline">\(x_t\)</span> ，相比于 LSTM 只有2个输入</li><li><strong>门控状态：</strong>其中 <span class="math inline">\(r\)</span> 控制重置的门控（reset gate）， <span class="math inline">\(z\)</span> 为控制更新的门控（update gate），update gate的作用类似于input gate和forget gate</li></ul><p><span class="math display">\[ \begin{aligned} &amp;z_t=\sigma\left(W_z \cdot\left[h_{t-1}, x_t\right]\right) \\ &amp;r_t=\sigma\left(W_r \cdot\left[h_{t-1}, x_t\right]\right) \\ \end{aligned} \]</span></p><ul><li><p><strong>新的候选状态：</strong> <span class="math display">\[ \hat{h}_t=\tanh \left(W \cdot\left[r_t* h_{t-1}, x_t\right]\right) \\ \]</span></p></li><li><p><strong>更新状态：</strong>同时进行了遗忘、记忆两个步骤 <span class="math display">\[ h_t=\left(1-z_t\right) * h_{t-1}+z_t * \hat{h}_t \]</span></p><ul><li>特点在于：<strong>使用了同一个门控 z 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）</strong>。</li><li>遗忘 <span class="math inline">\(z\)</span> 和选择 <span class="math inline">\(1−z\)</span> 是联动的。对于传递进来的维度信息，会进行选择性遗忘，遗忘了多少权重 <span class="math inline">\(z\)</span>，就使用包含当前输入的 <span class="math inline">\(\hat{h}_t\)</span> 中所对应的权重进行弥补 <span class="math inline">\(1−z\)</span>。以保持一种”恒定“状态。</li></ul></li></ul><h3 id="应用">2.4 应用</h3><ul><li>序列到类别模式</li><li>同步的序列到序列模式</li><li>异步的序列到序列模式</li></ul></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a> <span>></span> <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a></span></span></div></div><div class="license-box my-3"><div class="license-title"><div>ML学习笔记 #01 CNN &amp; RNN</div><div>https://yiwen-ding.github.io/ML-note1-cnn-rnn</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>CindyWen</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2022年11月29日</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/ML-note2-gnn" title="ML学习笔记 #02 GNN"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">ML学习笔记 #02 GNN</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/ML-note0-pytorch" title="ML学习笔记 #00 PyTorch"><span class="hidden-mobile">ML学习笔记 #00 PyTorch</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"7Ga9gCkoL9V2jdNQgXHdzud4-gzGzoHsz","appKey":"FGdWtMdPhO4fjMsxnDlriMuM","path":"window.location.pathname","placeholder":"欢迎评论:)","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":true,"recordIP":true,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>